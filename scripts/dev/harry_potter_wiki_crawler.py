#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºçˆ¬è™«
ä» https://harrypotter.fandom.com/zh çˆ¬å–è§’è‰²å’Œä¸–ç•Œè§‚æ•°æ®

æ³¨æ„: è¯¥ç»´åŸºä½¿ç”¨ CC-BY-SA åè®®ï¼Œçˆ¬å–çš„æ•°æ®éœ€è¦éµå¾ªç›¸åŒåè®®
"""

import requests
import time
import json
import re
import os
from urllib.parse import urljoin, urlparse, quote
from bs4 import BeautifulSoup
from dataclasses import dataclass
from typing import List, Dict, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class WikiPage:
    """ç»´åŸºé¡µé¢æ•°æ®ç»“æ„"""
    title: str
    url: str
    content: str
    category: str
    tags: List[str]
    importance: int = 5

class HarryPotterWikiCrawler:
    """å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºçˆ¬è™«"""
    
    def __init__(self):
        self.base_url = "https://harrypotter.fandom.com"
        self.zh_base = f"{self.base_url}/zh"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "data/harry_potter_wiki"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å·²çˆ¬å–é¡µé¢è®°å½• (é¿å…é‡å¤)
        self.crawled_urls = set()
        
        # çˆ¬å–ç­–ç•¥é…ç½®
        self.crawl_config = {
            # ä¸»è¦è§’è‰² (é«˜ä¼˜å…ˆçº§)
            "main_characters": {
                "urls": [
                    "/wiki/å“ˆåˆ©Â·æ³¢ç‰¹",
                    "/wiki/èµ«æ•Â·æ ¼å…°æ°", 
                    "/wiki/ç½—æ©Â·éŸ¦æ–¯è±",
                    "/wiki/é˜¿ä¸æ€Â·é‚“å¸ƒåˆ©å¤š",
                    "/wiki/è¥¿å¼—å‹’æ–¯Â·æ–¯å†…æ™®",
                    "/wiki/ä¼åœ°é­”",
                    "/wiki/å°å¤©ç‹¼æ˜ŸÂ·å¸ƒè±å…‹",
                    "/wiki/å¢å¹³",
                    "/wiki/é‡‘å¦®Â·éŸ¦æ–¯è±",
                    "/wiki/çº³å¨Â·éš†å·´é¡¿",
                    "/wiki/å¢å¨œÂ·æ´›å¤«å¤å¾·"
                ],
                "importance": 10,
                "category": "MAIN_CHARACTER"
            },
            
            # éœæ ¼æ²ƒèŒ¨ç›¸å…³ (é«˜ä¼˜å…ˆçº§)
            "hogwarts": {
                "urls": [
                    "/wiki/éœæ ¼æ²ƒèŒ¨é­”æ³•å­¦æ ¡",
                    "/wiki/æ ¼å…°èŠ¬å¤š",
                    "/wiki/æ–¯è±ç‰¹æ—", 
                    "/wiki/æ‹‰æ–‡å…‹åŠ³",
                    "/wiki/èµ«å¥‡å¸•å¥‡",
                    "/wiki/åˆ†é™¢å¸½",
                    "/wiki/éœæ ¼æ²ƒèŒ¨ç‰¹å¿«åˆ—è½¦",
                    "/wiki/å¤§ç¤¼å ‚",
                    "/wiki/æ ¼å…°èŠ¬å¤šå¡”",
                    "/wiki/åœ°ä¸‹å®¤"
                ],
                "importance": 9,
                "category": "HOGWARTS"
            },
            
            # é­”æ³•ä¸–ç•Œè§‚ (ä¸­ä¼˜å…ˆçº§)
            "magic_world": {
                "urls": [
                    "/wiki/é­”æ³•",
                    "/wiki/é­åœ°å¥‡",
                    "/wiki/å®ˆæŠ¤ç¥å’’",
                    "/wiki/é˜¿ç“¦è¾¾ç´¢å‘½å’’",
                    "/wiki/é’»å¿ƒå‰œéª¨",
                    "/wiki/é­‚å™¨", 
                    "/wiki/å‡¤å‡°ç¤¾",
                    "/wiki/é£Ÿæ­»å¾’",
                    "/wiki/é­”æ³•éƒ¨",
                    "/wiki/å¯¹è§’å··",
                    "/wiki/éœæ ¼è«å¾·"
                ],
                "importance": 8,
                "category": "MAGIC_WORLD"
            },
            
            # é‡è¦äº‹ä»¶ (ä¸­ä¼˜å…ˆçº§)  
            "events": {
                "urls": [
                    "/wiki/éœæ ¼æ²ƒèŒ¨å¤§æˆ˜",
                    "/wiki/ä¸‰å¼ºäº‰éœ¸èµ›",
                    "/wiki/é­åœ°å¥‡ä¸–ç•Œæ¯",
                    "/wiki/ç¥ç§˜äº‹åŠ¡å¸ä¹‹æˆ˜"
                ],
                "importance": 7,
                "category": "EVENTS"
            }
        }
    
    def get_page_content(self, url: str) -> Optional[BeautifulSoup]:
        """è·å–é¡µé¢å†…å®¹"""
        # ç¡®ä¿URLä»¥/å¼€å¤´è¿›è¡Œæ­£ç¡®çš„è·¯å¾„æ‹¼æ¥
        if not url.startswith('/'):
            url = '/' + url
        full_url = self.zh_base + url
        
        if full_url in self.crawled_urls:
            logger.info(f"é¡µé¢å·²çˆ¬å–ï¼Œè·³è¿‡: {url}")
            return None
            
        try:
            logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
            response = self.session.get(full_url, timeout=10)
            response.raise_for_status()
            
            # å¼ºåˆ¶è®¾ç½®ç¼–ç ä¸ºUTF-8
            response.encoding = 'utf-8'
            
            # æ·»åŠ åˆ°å·²çˆ¬å–è®°å½•
            self.crawled_urls.add(full_url)
            
            # ä½¿ç”¨response.textè€Œä¸æ˜¯response.content
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup
            
        except requests.RequestException as e:
            logger.error(f"çˆ¬å–å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    def extract_page_data(self, soup: BeautifulSoup, url: str, category: str, importance: int) -> Optional[WikiPage]:
        """æå–é¡µé¢æ•°æ®"""
        try:
            # è·å–é¡µé¢æ ‡é¢˜
            title_elem = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
            title = title_elem.get_text().strip() if title_elem else "æœªçŸ¥æ ‡é¢˜"
            
            # è·å–ä¸»è¦å†…å®¹åŒºåŸŸ (ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨)
            selectors = [
                ('div[id="mw-content-text"]', 'MWå†…å®¹åŒºåŸŸ'),
                ('div.main-container', 'ä¸»å®¹å™¨'),
                ('div#content', 'å†…å®¹åŒºåŸŸ'),
                ('div.page-content', 'é¡µé¢å†…å®¹'),
                ('div.mw-body-content', 'MWä¸»ä½“å†…å®¹')
            ]
            
            content_div = None
            for selector, desc in selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' ({desc}) æ‰¾åˆ°å†…å®¹åŒºåŸŸ")
                    break
            
            if not content_div:
                # å°è¯•findæ–¹æ³•
                content_div = soup.find('div', id='mw-content-text')
                if content_div:
                    logger.info(f"ä½¿ç”¨findæ–¹æ³•æ‰¾åˆ°mw-content-text")
                else:
                    logger.warning(f"æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ: {url}")
                    
                    # å°è¯•ç›´æ¥è·å–æ®µè½
                    paragraphs = soup.find_all('p')
                    if paragraphs and len(paragraphs) > 10:
                        logger.info(f"æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½ï¼Œå°è¯•ç›´æ¥æå–")
                        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„å†…å®¹div
                        content_div = soup.new_tag('div')
                        for p in paragraphs:
                            content_div.append(p)
                    else:
                        logger.warning(f"ä¹Ÿæœªæ‰¾åˆ°è¶³å¤Ÿçš„æ®µè½å†…å®¹ ({len(paragraphs) if paragraphs else 0} ä¸ª)")
                        return None
            
            # æ¸…ç†å†…å®¹
            content = self.clean_content(content_div)
            
            # æå–æ ‡ç­¾
            tags = self.extract_tags(soup, content)
            
            return WikiPage(
                title=title,
                url=url,
                content=content,
                category=category,
                tags=tags,
                importance=importance
            )
            
        except Exception as e:
            logger.error(f"æ•°æ®æå–å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    def clean_content(self, content_div) -> str:
        """æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for elem in content_div.find_all(['script', 'style', 'nav', 'footer']):
            elem.decompose()
        
        # ç§»é™¤å¼•ç”¨æ ‡è®° [1], [2] ç­‰
        for elem in content_div.find_all('sup', {'class': 'reference'}):
            elem.decompose()
        
        # ç§»é™¤ç¼–è¾‘é“¾æ¥
        for elem in content_div.find_all('span', {'class': 'mw-editsection'}):
            elem.decompose()
            
        # æå–çº¯æ–‡æœ¬ï¼Œä¿ç•™æ®µè½ç»“æ„
        paragraphs = []
        for p in content_div.find_all(['p', 'h2', 'h3', 'h4', 'li']):
            text = p.get_text().strip()
            if text and len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                paragraphs.append(text)
        
        # åˆå¹¶æ®µè½
        content = '\n\n'.join(paragraphs)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r' {2,}', ' ', content)
        
        return content.strip()
    
    def extract_tags(self, soup: BeautifulSoup, content: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        tags = []
        
        # ä»åˆ†ç±»ä¸­æå–æ ‡ç­¾
        category_links = soup.find_all('a', href=re.compile(r'/wiki/Category:'))
        for link in category_links:
            tag = link.get_text().strip()
            if tag and len(tag) < 50:  # é¿å…å¤ªé•¿çš„æ ‡ç­¾
                tags.append(tag)
        
        # ä»å†…å®¹ä¸­æå–å…³é”®è¯
        keywords = self.extract_keywords(content)
        tags.extend(keywords)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        tags = list(set(tags))[:10]
        
        return tags
    
    def extract_keywords(self, content: str) -> List[str]:
        """ä»å†…å®¹ä¸­æå–å…³é”®è¯"""
        # å“ˆåˆ©Â·æ³¢ç‰¹ä¸–ç•Œçš„å…³é”®è¯
        hp_keywords = [
            "æ ¼å…°èŠ¬å¤š", "æ–¯è±ç‰¹æ—", "æ‹‰æ–‡å…‹åŠ³", "èµ«å¥‡å¸•å¥‡",
            "éœæ ¼æ²ƒèŒ¨", "é­åœ°å¥‡", "é­”æ³•", "å·«å¸ˆ", "éº»ç“œ",
            "ä¼åœ°é­”", "é£Ÿæ­»å¾’", "å‡¤å‡°ç¤¾", "é­”æ³•éƒ¨",
            "é­‚å™¨", "å®ˆæŠ¤ç¥", "é˜¿ç“¦è¾¾ç´¢å‘½", "é’»å¿ƒå‰œéª¨",
            "å¯¹è§’å··", "éœæ ¼è«å¾·", "é˜¿å…¹å¡ç­"
        ]
        
        found_keywords = []
        content_lower = content.lower()
        
        for keyword in hp_keywords:
            if keyword in content:
                found_keywords.append(keyword)
        
        return found_keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯
    
    def save_page_data(self, page_data: WikiPage):
        """ä¿å­˜é¡µé¢æ•°æ®åˆ°æ–‡ä»¶"""
        # åˆ›å»ºåˆ†ç±»ç›®å½•
        category_dir = os.path.join(self.output_dir, page_data.category.lower())
        os.makedirs(category_dir, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å (å¤„ç†ç‰¹æ®Šå­—ç¬¦)
        safe_title = re.sub(r'[^\w\s-]', '', page_data.title)
        safe_title = re.sub(r'[\s]+', '_', safe_title)
        filename = f"{safe_title}.json"
        
        filepath = os.path.join(category_dir, filename)
        
        # è½¬æ¢ä¸ºå­—å…¸
        data = {
            "title": page_data.title,
            "url": page_data.url,
            "content": page_data.content,
            "category": page_data.category,
            "tags": page_data.tags,
            "importance": page_data.importance,
            "source": "å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸº",
            "source_url": f"https://harrypotter.fandom.com/zh{page_data.url}",
            "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·²ä¿å­˜: {filepath}")
    
    def crawl_by_category(self, category_name: str, config: Dict):
        """æŒ‰åˆ†ç±»çˆ¬å–"""
        logger.info(f"å¼€å§‹çˆ¬å–åˆ†ç±»: {category_name}")
        
        urls = config["urls"]
        importance = config["importance"]
        category = config["category"]
        
        crawled_count = 0
        
        for url in urls:
            try:
                # è·å–é¡µé¢å†…å®¹
                soup = self.get_page_content(url)
                if not soup:
                    continue
                
                # æå–æ•°æ®
                page_data = self.extract_page_data(soup, url, category, importance)
                if not page_data:
                    continue
                
                # ä¿å­˜æ•°æ®
                self.save_page_data(page_data)
                crawled_count += 1
                
                # ç¤¼è²Œæ€§å»¶è¿Ÿ
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"å¤„ç†é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"åˆ†ç±» {category_name} çˆ¬å–å®Œæˆï¼ŒæˆåŠŸçˆ¬å– {crawled_count} é¡µ")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        logger.info("å¼€å§‹çˆ¬å–å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºæ•°æ®...")
        
        total_crawled = 0
        
        # æŒ‰ä¼˜å…ˆçº§çˆ¬å–
        for category_name, config in self.crawl_config.items():
            self.crawl_by_category(category_name, config)
            
            # ç»Ÿè®¡
            total_crawled += len(config["urls"])
            
            # åˆ†ç±»é—´å»¶è¿Ÿ
            time.sleep(2)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼æ€»å…±å¤„ç† {total_crawled} ä¸ªé¡µé¢")
        logger.info(f"æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            "crawl_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_pages": len(self.crawled_urls),
            "categories": {},
            "output_directory": self.output_dir
        }
        
        # ç»Ÿè®¡å„åˆ†ç±»
        for category_name, config in self.crawl_config.items():
            report["categories"][category_name] = {
                "count": len(config["urls"]),
                "importance": config["importance"],
                "category": config["category"]
            }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, "crawl_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§™â€â™‚ï¸ å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºçˆ¬è™«")
    print("=" * 50)
    
    crawler = HarryPotterWikiCrawler()
    
    try:
        crawler.run()
        print("\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {crawler.output_dir}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
        logger.error(f"çˆ¬å–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
