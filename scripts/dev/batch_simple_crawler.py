#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ç®€åŒ–çˆ¬è™«
åŸºäºéªŒè¯å¯å·¥ä½œçš„ç®€å•çˆ¬è™«é€»è¾‘ï¼Œæ‰¹é‡çˆ¬å–å¤šä¸ªé¡µé¢
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from typing import List, Dict

class BatchSimpleCrawler:
    """æ‰¹é‡ç®€åŒ–çˆ¬è™«"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.base_url = "https://harrypotter.fandom.com/zh"
        self.output_dir = "batch_crawled_data"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¦çˆ¬å–çš„é¡µé¢åˆ—è¡¨
        self.pages_to_crawl = [
            # ä¸»è¦è§’è‰²
            {
                "url": "/wiki/%E5%93%88%E5%88%A9%C2%B7%E6%B3%A2%E7%89%B9",
                "title": "å“ˆåˆ©Â·æ³¢ç‰¹",
                "category": "MAIN_CHARACTER",
                "importance": 10
            },
            {
                "url": "/wiki/%E8%B5%AB%E6%95%8F%C2%B7%E6%A0%BC%E5%85%B0%E6%9D%B0",
                "title": "èµ«æ•Â·æ ¼å…°æ°", 
                "category": "MAIN_CHARACTER",
                "importance": 10
            },
            {
                "url": "/wiki/%E7%BD%97%E6%81%A9%C2%B7%E9%9F%A6%E6%96%AF%E8%8E%B1",
                "title": "ç½—æ©Â·éŸ¦æ–¯è±",
                "category": "MAIN_CHARACTER", 
                "importance": 10
            },
            # éœæ ¼æ²ƒèŒ¨ç›¸å…³
            {
                "url": "/wiki/%E5%88%86%E9%99%A2%E5%B8%BD",
                "title": "åˆ†é™¢å¸½",
                "category": "HOGWARTS",
                "importance": 9
            },
            {
                "url": "/wiki/%E9%9C%8D%E6%A0%BC%E6%B2%83%E8%8C%A8%E9%AD%94%E6%B3%95%E5%AD%A6%E6%A0%A1",
                "title": "éœæ ¼æ²ƒèŒ¨é­”æ³•å­¦æ ¡",
                "category": "HOGWARTS",
                "importance": 9
            },
            {
                "url": "/wiki/%E6%A0%BC%E5%85%B0%E8%8A%AC%E5%A4%9A%E5%AD%A6%E9%99%A2",
                "title": "æ ¼å…°èŠ¬å¤šå­¦é™¢",
                "category": "HOGWARTS",
                "importance": 8
            },
            # é­”æ³•ä¸–ç•Œ
            {
                "url": "/wiki/%E9%AD%81%E5%9C%B0%E5%A5%87",
                "title": "é­åœ°å¥‡",
                "category": "MAGIC_WORLD",
                "importance": 8
            },
            {
                "url": "/wiki/%E9%98%BF%E7%93%A6%E8%BE%BE%E7%B4%A2%E5%91%BD%E5%92%92",
                "title": "é˜¿ç“¦è¾¾ç´¢å‘½å’’",
                "category": "MAGIC_WORLD",
                "importance": 8
            }
        ]
    
    def crawl_single_page(self, page_info: Dict) -> bool:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        url = page_info["url"]
        full_url = self.base_url + url
        
        try:
            print(f"ğŸ” çˆ¬å–: {page_info['title']} ({url})")
            
            response = requests.get(full_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_elem = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
            title = title_elem.get_text().strip() if title_elem else page_info["title"]
            
            # æå–å†…å®¹
            content_div = soup.find('div', id='mw-content-text')
            
            if not content_div:
                print(f"  âŒ æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ")
                return False
            
            paragraphs = content_div.find_all('p')
            
            # æå–æœ‰æ•ˆæ®µè½
            valid_paragraphs = []
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 20:
                    valid_paragraphs.append(text)
            
            if not valid_paragraphs:
                print(f"  âŒ æ²¡æœ‰æœ‰æ•ˆæ®µè½")
                return False
            
            # åˆå¹¶æ‰€æœ‰æ®µè½
            content = '\n\n'.join(valid_paragraphs)
            
            # æå–å…³é”®è¯æ ‡ç­¾
            tags = self.extract_tags(content, page_info["category"])
            
            # åˆ›å»ºæ•°æ®
            data = {
                "characterId": 1,  # é»˜è®¤å“ˆåˆ©Â·æ³¢ç‰¹
                "title": title,
                "content": content,
                "knowledgeType": self.map_category_to_knowledge_type(page_info["category"]),
                "importance": page_info["importance"],
                "source": "å“ˆåˆ©Â·æ³¢ç‰¹ä¸­æ–‡ç»´åŸº",
                "sourceUrl": full_url,
                "tags": tags
            }
            
            # ä¿å­˜æ•°æ®
            safe_filename = self.make_safe_filename(title)
            output_file = os.path.join(self.output_dir, f"{safe_filename}.json")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ… æˆåŠŸä¿å­˜: {len(valid_paragraphs)} æ®µè½, {len(content)} å­—ç¬¦")
            return True
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
            return False
    
    def extract_tags(self, content: str, category: str) -> List[str]:
        """æå–æ ‡ç­¾"""
        # å“ˆåˆ©Â·æ³¢ç‰¹ç›¸å…³å…³é”®è¯
        keywords = [
            "éœæ ¼æ²ƒèŒ¨", "æ ¼å…°èŠ¬å¤š", "æ–¯è±ç‰¹æ—", "æ‹‰æ–‡å…‹åŠ³", "èµ«å¥‡å¸•å¥‡",
            "é­åœ°å¥‡", "åˆ†é™¢å¸½", "é­”æ³•", "å·«å¸ˆ", "é­”æ–", 
            "å®ˆæŠ¤ç¥", "ä¼åœ°é­”", "é£Ÿæ­»å¾’", "å‡¤å‡°ç¤¾", "é­”æ³•éƒ¨"
        ]
        
        found_tags = []
        for keyword in keywords:
            if keyword in content:
                found_tags.append(keyword)
        
        # æ·»åŠ åˆ†ç±»æ ‡ç­¾
        if category == "MAIN_CHARACTER":
            found_tags.append("ä¸»è¦è§’è‰²")
        elif category == "HOGWARTS":
            found_tags.append("éœæ ¼æ²ƒèŒ¨")
        elif category == "MAGIC_WORLD":
            found_tags.append("é­”æ³•ä¸–ç•Œ")
        
        return found_tags[:8]  # æœ€å¤š8ä¸ªæ ‡ç­¾
    
    def map_category_to_knowledge_type(self, category: str) -> str:
        """æ˜ å°„åˆ†ç±»åˆ°çŸ¥è¯†ç±»å‹"""
        mapping = {
            "MAIN_CHARACTER": "PERSONALITY",
            "HOGWARTS": "BASIC_INFO", 
            "MAGIC_WORLD": "KNOWLEDGE",
            "EVENTS": "EVENTS"
        }
        return mapping.get(category, "BASIC_INFO")
    
    def make_safe_filename(self, title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        import re
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        safe_name = re.sub(r'[^\w\s-]', '', title)
        safe_name = re.sub(r'\s+', '_', safe_name)
        return safe_name[:50]  # é™åˆ¶é•¿åº¦
    
    def crawl_all(self) -> Dict[str, int]:
        """æ‰¹é‡çˆ¬å–æ‰€æœ‰é¡µé¢"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(self.pages_to_crawl)} ä¸ªé¡µé¢...")
        
        success_count = 0
        failed_count = 0
        
        for i, page_info in enumerate(self.pages_to_crawl):
            try:
                if self.crawl_single_page(page_info):
                    success_count += 1
                else:
                    failed_count += 1
                
                # è¿›åº¦æ˜¾ç¤º
                progress = (i + 1) / len(self.pages_to_crawl) * 100
                print(f"ğŸ“Š è¿›åº¦: {i + 1}/{len(self.pages_to_crawl)} ({progress:.1f}%)")
                
                # ç¤¼è²Œæ€§å»¶è¿Ÿ
                time.sleep(1)
                
            except KeyboardInterrupt:
                print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†é¡µé¢æ—¶å‡ºé”™: {e}")
                failed_count += 1
                continue
        
        return {
            "total": len(self.pages_to_crawl),
            "success": success_count,
            "failed": failed_count
        }
    
    def generate_import_data(self) -> str:
        """ç”Ÿæˆå¯¼å…¥æ•°æ®"""
        print("\nğŸ“¦ ç”Ÿæˆæ‰¹é‡å¯¼å…¥æ•°æ®...")
        
        all_data = []
        
        # è¯»å–æ‰€æœ‰JSONæ–‡ä»¶
        for filename in os.listdir(self.output_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(self.output_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    all_data.append(data)
        
        # ä¿å­˜åˆå¹¶æ•°æ®
        output_file = "harry_potter_knowledge_batch.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜: {output_file}")
        print(f"ğŸ“Š æ€»è®¡ {len(all_data)} ä¸ªçŸ¥è¯†æ¡ç›®")
        
        return output_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§™â€â™‚ï¸ å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºæ‰¹é‡çˆ¬è™«")
    print("=" * 50)
    
    crawler = BatchSimpleCrawler()
    
    try:
        # çˆ¬å–æ•°æ®
        result = crawler.crawl_all()
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"æ€»é¡µé¢æ•°: {result['total']}")
        print(f"æˆåŠŸçˆ¬å–: {result['success']}")
        print(f"çˆ¬å–å¤±è´¥: {result['failed']}")
        print(f"æˆåŠŸç‡: {result['success']/result['total']*100:.1f}%")
        
        if result['success'] > 0:
            # ç”Ÿæˆå¯¼å…¥æ•°æ®
            import_file = crawler.generate_import_data()
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {crawler.output_dir}")
            print(f"ğŸ“„ å¯¼å…¥æ–‡ä»¶: {import_file}")
            print(f"\nğŸš€ ä¸‹ä¸€æ­¥: å°†æ•°æ®å¯¼å…¥åˆ°RAGç³»ç»Ÿ")
            print(f"å‘½ä»¤: python3 import_to_rag.py {import_file}")
        else:
            print(f"\nâŒ æ²¡æœ‰æˆåŠŸçˆ¬å–ä»»ä½•æ•°æ®")
    
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
