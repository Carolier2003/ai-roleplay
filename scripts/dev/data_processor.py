#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºæ•°æ®å¤„ç†å™¨
å°†çˆ¬å–çš„JSONæ•°æ®å¤„ç†æˆé€‚åˆRAGç³»ç»Ÿçš„æ ¼å¼
"""

import json
import os
import re
from typing import List, Dict, Any
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProcessedKnowledge:
    """å¤„ç†åçš„çŸ¥è¯†æ•°æ®"""
    title: str
    content: str
    knowledge_type: str
    importance: int
    source: str
    source_url: str
    tags: List[str]
    character_id: int = 1  # é»˜è®¤ä¸ºå“ˆåˆ©Â·æ³¢ç‰¹

class HarryPotterDataProcessor:
    """å“ˆåˆ©Â·æ³¢ç‰¹æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_dir: str = "data/harry_potter_wiki"):
        self.data_dir = data_dir
        self.processed_data = []
        
        # çŸ¥è¯†ç±»å‹æ˜ å°„
        self.knowledge_type_mapping = {
            "MAIN_CHARACTER": "PERSONALITY",     # ä¸»è¦è§’è‰² -> æ€§æ ¼ç‰¹å¾
            "HOGWARTS": "BASIC_INFO",           # éœæ ¼æ²ƒèŒ¨ -> åŸºæœ¬ä¿¡æ¯
            "MAGIC_WORLD": "KNOWLEDGE",         # é­”æ³•ä¸–ç•Œ -> ä¸“ä¸šçŸ¥è¯†
            "EVENTS": "EVENTS",                 # äº‹ä»¶ -> é‡è¦äº‹ä»¶
            "RELATIONSHIPS": "RELATIONSHIPS",    # å…³ç³» -> äººé™…å…³ç³»
            "ABILITIES": "ABILITIES",           # èƒ½åŠ› -> èƒ½åŠ›æŠ€èƒ½
            "QUOTES": "QUOTES"                  # è¯­å½• -> ç»å…¸è¯­å½•
        }
    
    def load_crawled_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½çˆ¬å–çš„æ•°æ®"""
        all_data = []
        
        for root, dirs, files in os.walk(self.data_dir):
            for file in files:
                if file.endswith('.json') and file != 'crawl_report.json':
                    filepath = os.path.join(root, file)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            all_data.append(data)
                    except Exception as e:
                        logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {e}")
        
        logger.info(f"åŠ è½½äº† {len(all_data)} ä¸ªæ•°æ®æ–‡ä»¶")
        return all_data
    
    def split_content(self, content: str, max_chunk_size: int = 500) -> List[str]:
        """å°†é•¿å†…å®¹æ‹†åˆ†ä¸ºå¤šä¸ªå—"""
        if len(content) <= max_chunk_size:
            return [content]
        
        # æŒ‰æ®µè½æ‹†åˆ†
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) > max_chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    # å•ä¸ªæ®µè½å¤ªé•¿ï¼Œå¼ºåˆ¶æ‹†åˆ†
                    words = paragraph.split('ã€‚')
                    for word in words:
                        if len(current_chunk) + len(word) > max_chunk_size:
                            if current_chunk:
                                chunks.append(current_chunk.strip())
                            current_chunk = word
                        else:
                            current_chunk += word + "ã€‚"
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def enhance_content_with_context(self, data: Dict[str, Any]) -> str:
        """ä¸ºå†…å®¹æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        title = data.get('title', '')
        content = data.get('content', '')
        category = data.get('category', '')
        tags = data.get('tags', [])
        
        # æ·»åŠ æ ‡é¢˜ä½œä¸ºä¸Šä¸‹æ–‡
        enhanced_content = f"ã€{title}ã€‘\n\n{content}"
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œæ·»åŠ å…³é”®è¯è¯´æ˜
        if tags:
            relevant_tags = [tag for tag in tags if len(tag) < 20][:5]
            if relevant_tags:
                enhanced_content += f"\n\nå…³é”®è¯: {', '.join(relevant_tags)}"
        
        return enhanced_content
    
    def determine_importance(self, data: Dict[str, Any]) -> int:
        """æ™ºèƒ½ç¡®å®šé‡è¦æ€§ç­‰çº§"""
        base_importance = data.get('importance', 5)
        title = data.get('title', '').lower()
        content = data.get('content', '')
        
        # é‡è¦è§’è‰²æå‡é‡è¦æ€§
        important_characters = ['å“ˆåˆ©Â·æ³¢ç‰¹', 'èµ«æ•', 'ç½—æ©', 'é‚“å¸ƒåˆ©å¤š', 'ä¼åœ°é­”', 'æ–¯å†…æ™®']
        for char in important_characters:
            if char in title:
                base_importance = min(10, base_importance + 2)
                break
        
        # æ ¸å¿ƒæ¦‚å¿µæå‡é‡è¦æ€§
        core_concepts = ['éœæ ¼æ²ƒèŒ¨', 'åˆ†é™¢å¸½', 'é­åœ°å¥‡', 'é­‚å™¨', 'å®ˆæŠ¤ç¥']
        for concept in core_concepts:
            if concept in title:
                base_importance = min(10, base_importance + 1)
                break
        
        # å†…å®¹é•¿åº¦å½±å“é‡è¦æ€§
        if len(content) > 1000:
            base_importance = min(10, base_importance + 1)
        elif len(content) < 200:
            base_importance = max(1, base_importance - 1)
        
        return base_importance
    
    def process_data(self) -> List[ProcessedKnowledge]:
        """å¤„ç†æ•°æ®"""
        raw_data = self.load_crawled_data()
        processed_knowledge = []
        
        for data in raw_data:
            try:
                # è·å–åŸºæœ¬ä¿¡æ¯
                title = data.get('title', 'æœªçŸ¥æ ‡é¢˜')
                content = data.get('content', '')
                category = data.get('category', 'BASIC_INFO')
                tags = data.get('tags', [])
                source_url = data.get('source_url', '')
                
                # è·³è¿‡ç©ºå†…å®¹
                if len(content.strip()) < 50:
                    logger.warning(f"å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {title}")
                    continue
                
                # å¢å¼ºå†…å®¹
                enhanced_content = self.enhance_content_with_context(data)
                
                # æ‹†åˆ†é•¿å†…å®¹
                content_chunks = self.split_content(enhanced_content)
                
                # æ˜ å°„çŸ¥è¯†ç±»å‹
                knowledge_type = self.knowledge_type_mapping.get(category, 'BASIC_INFO')
                
                # ç¡®å®šé‡è¦æ€§
                importance = self.determine_importance(data)
                
                # ä¸ºæ¯ä¸ªå—åˆ›å»ºçŸ¥è¯†æ¡ç›®
                for i, chunk in enumerate(content_chunks):
                    chunk_title = f"{title}" if len(content_chunks) == 1 else f"{title} (ç¬¬{i+1}éƒ¨åˆ†)"
                    
                    processed = ProcessedKnowledge(
                        title=chunk_title,
                        content=chunk,
                        knowledge_type=knowledge_type,
                        importance=importance,
                        source="å“ˆåˆ©Â·æ³¢ç‰¹ä¸­æ–‡ç»´åŸº",
                        source_url=source_url,
                        tags=tags[:5]  # é™åˆ¶æ ‡ç­¾æ•°é‡
                    )
                    
                    processed_knowledge.append(processed)
                
                logger.info(f"å¤„ç†å®Œæˆ: {title} -> {len(content_chunks)} ä¸ªçŸ¥è¯†å—")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ•°æ®å¤±è´¥: {data.get('title', 'unknown')}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(processed_knowledge)} ä¸ªçŸ¥è¯†æ¡ç›®")
        return processed_knowledge
    
    def save_processed_data(self, processed_data: List[ProcessedKnowledge], output_file: str = "processed_harry_potter_knowledge.json"):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        output_path = os.path.join(os.path.dirname(self.data_dir), output_file)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_list = []
        for item in processed_data:
            data_dict = {
                "characterId": item.character_id,
                "title": item.title,
                "content": item.content,
                "knowledgeType": item.knowledge_type,
                "importance": item.importance,
                "source": item.source,
                "sourceUrl": item.source_url,
                "tags": item.tags
            }
            data_list.append(data_dict)
        
        # ä¿å­˜ä¸ºJSON
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_processing_report(processed_data, output_path)
        
        return output_path
    
    def generate_processing_report(self, processed_data: List[ProcessedKnowledge], output_path: str):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "æ€»çŸ¥è¯†æ¡ç›®æ•°": len(processed_data),
            "æŒ‰ç±»å‹ç»Ÿè®¡": {},
            "æŒ‰é‡è¦æ€§ç»Ÿè®¡": {},
            "å¹³å‡å†…å®¹é•¿åº¦": 0,
            "è¾“å‡ºæ–‡ä»¶": output_path
        }
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        for item in processed_data:
            knowledge_type = item.knowledge_type
            stats["æŒ‰ç±»å‹ç»Ÿè®¡"][knowledge_type] = stats["æŒ‰ç±»å‹ç»Ÿè®¡"].get(knowledge_type, 0) + 1
        
        # æŒ‰é‡è¦æ€§ç»Ÿè®¡
        for item in processed_data:
            importance = item.importance
            stats["æŒ‰é‡è¦æ€§ç»Ÿè®¡"][f"é‡è¦æ€§{importance}"] = stats["æŒ‰é‡è¦æ€§ç»Ÿè®¡"].get(f"é‡è¦æ€§{importance}", 0) + 1
        
        # å¹³å‡é•¿åº¦
        total_length = sum(len(item.content) for item in processed_data)
        stats["å¹³å‡å†…å®¹é•¿åº¦"] = total_length // len(processed_data) if processed_data else 0
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = output_path.replace('.json', '_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # æ‰“å°ç®€è¦ç»Ÿè®¡
        print("\nğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 40)
        print(f"æ€»çŸ¥è¯†æ¡ç›®æ•°: {stats['æ€»çŸ¥è¯†æ¡ç›®æ•°']}")
        print(f"å¹³å‡å†…å®¹é•¿åº¦: {stats['å¹³å‡å†…å®¹é•¿åº¦']} å­—ç¬¦")
        print("\næŒ‰ç±»å‹åˆ†å¸ƒ:")
        for k, v in stats["æŒ‰ç±»å‹ç»Ÿè®¡"].items():
            print(f"  {k}: {v} æ¡")
        print("\næŒ‰é‡è¦æ€§åˆ†å¸ƒ:")
        for k, v in sorted(stats["æŒ‰é‡è¦æ€§ç»Ÿè®¡"].items()):
            print(f"  {k}: {v} æ¡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“š å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºæ•°æ®å¤„ç†å™¨")
    print("=" * 50)
    
    processor = HarryPotterDataProcessor()
    
    try:
        # å¤„ç†æ•°æ®
        processed_data = processor.process_data()
        
        if not processed_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«è„šæœ¬")
            return
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        output_file = processor.save_processed_data(processed_data)
        
        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
        print(f"ğŸ“ å…±å¤„ç† {len(processed_data)} ä¸ªçŸ¥è¯†æ¡ç›®")
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        logger.error(f"å¤„ç†å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
