#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç®€å•çš„çˆ¬è™«æµ‹è¯•
ç›´æ¥å¤åˆ¶æˆåŠŸçš„è°ƒè¯•ä»£ç 
"""

import requests
from bs4 import BeautifulSoup
import json
import os

def simple_crawl_test():
    """æœ€ç®€å•çš„çˆ¬å–æµ‹è¯•"""
    url = "https://harrypotter.fandom.com/zh/wiki/%E5%88%86%E9%99%A2%E5%B8%BD"
    
    # ä½¿ç”¨ç›¸åŒçš„è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"ğŸ” çˆ¬å–: {url}")
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
        title_text = title.get_text().strip() if title else "æœªçŸ¥æ ‡é¢˜"
        
        # æå–å†…å®¹
        content_div = soup.find('div', id='mw-content-text')
        
        if content_div:
            print(f"âœ… æ‰¾åˆ°å†…å®¹åŒºåŸŸ")
            paragraphs = content_div.find_all('p')
            
            # æå–æœ‰æ•ˆæ®µè½
            valid_paragraphs = []
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 20:
                    valid_paragraphs.append(text)
            
            if valid_paragraphs:
                # åˆå¹¶æ‰€æœ‰æ®µè½
                content = '\n\n'.join(valid_paragraphs)
                
                # åˆ›å»ºæ•°æ®
                data = {
                    "title": title_text,
                    "url": "/wiki/%E5%88%86%E9%99%A2%E5%B8%BD",
                    "content": content,
                    "category": "TEST_DATA",
                    "tags": ["åˆ†é™¢å¸½", "éœæ ¼æ²ƒèŒ¨", "é­”æ³•"],
                    "importance": 9,
                    "source": "å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸº",
                    "source_url": url
                }
                
                # ä¿å­˜æ•°æ®
                os.makedirs("simple_test_data", exist_ok=True)
                with open("simple_test_data/sorting_hat.json", 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… æˆåŠŸçˆ¬å–å¹¶ä¿å­˜æ•°æ®")
                print(f"ğŸ“ æ ‡é¢˜: {title_text}")
                print(f"ğŸ“„ æ®µè½æ•°: {len(valid_paragraphs)}")
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:100]}...")
                
                return True
            else:
                print(f"âŒ æ²¡æœ‰æœ‰æ•ˆæ®µè½")
                return False
        else:
            print(f"âŒ æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ§ª æœ€ç®€å•çš„çˆ¬è™«æµ‹è¯•")
    print("=" * 40)
    if simple_crawl_test():
        print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼çˆ¬è™«é€»è¾‘æ²¡æœ‰é—®é¢˜")
        print("é—®é¢˜å¯èƒ½åœ¨äºå¤æ‚çš„çˆ¬è™«æ¡†æ¶ä¸­çš„æŸä¸ªç¯èŠ‚")
    else:
        print("\nâŒ åŸºç¡€çˆ¬å–ä¹Ÿå¤±è´¥äº†")
