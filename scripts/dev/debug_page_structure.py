#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•é¡µé¢ç»“æ„è„šæœ¬
æ£€æŸ¥å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºçš„å®é™…é¡µé¢ç»“æ„
"""

import requests
from bs4 import BeautifulSoup

def debug_page_structure():
    """è°ƒè¯•é¡µé¢ç»“æ„"""
    print("ğŸ” è°ƒè¯•å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºé¡µé¢ç»“æ„...")
    
    try:
        url = "https://harrypotter.fandom.com/zh/wiki/%E5%88%86%E9%99%A2%E5%B8%BD"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'N/A'}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„å†…å®¹åŒºåŸŸ
        print("\nğŸ” æŸ¥æ‰¾å†…å®¹åŒºåŸŸ...")
        
        # å°è¯•ä¸åŒçš„å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            ('div.mw-content-text', 'MediaWikiå†…å®¹åŒºåŸŸ'),
            ('div#content', 'IDä¸ºcontentçš„åŒºåŸŸ'),
            ('div.page-content', 'é¡µé¢å†…å®¹åŒºåŸŸ'),
            ('main', 'ä¸»è¦å†…å®¹åŒºåŸŸ'),
            ('article', 'æ–‡ç« åŒºåŸŸ'),
            ('div.WikiaPageContentWrapper', 'Wikiaé¡µé¢åŒ…è£…å™¨'),
            ('div.page-header__title', 'é¡µé¢æ ‡é¢˜'),
            ('div.portable-infobox', 'ä¿¡æ¯æ¡†'),
            ('div#mw-content-text', 'MWå†…å®¹æ–‡æœ¬')
        ]
        
        for selector, description in content_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"  âœ… æ‰¾åˆ° {description}: {len(elements)} ä¸ªå…ƒç´ ")
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå…ƒç´ çš„å‰100ä¸ªå­—ç¬¦
                first_element = elements[0]
                text = first_element.get_text().strip()[:100]
                print(f"     é¢„è§ˆ: {text}...")
            else:
                print(f"  âŒ æœªæ‰¾åˆ° {description}")
        
        # æŸ¥æ‰¾æ‰€æœ‰æ®µè½
        paragraphs = soup.find_all('p')
        print(f"\nğŸ“ æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
        
        if paragraphs:
            for i, p in enumerate(paragraphs[:5]):  # åªçœ‹å‰5ä¸ª
                text = p.get_text().strip()
                if len(text) > 20:
                    print(f"  æ®µè½ {i+1}: {text[:80]}...")
        
        # æŸ¥æ‰¾æ‰€æœ‰div classå±æ€§
        print(f"\nğŸ·ï¸  é¡µé¢ä¸­çš„ä¸»è¦divç±»å:")
        divs_with_class = soup.find_all('div', class_=True)
        class_names = set()
        for div in divs_with_class:
            for class_name in div.get('class', []):
                if 'content' in class_name.lower() or 'text' in class_name.lower() or 'body' in class_name.lower():
                    class_names.add(class_name)
        
        for class_name in sorted(class_names)[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {class_name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    debug_page_structure()
