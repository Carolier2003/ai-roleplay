#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ‰©å±•çˆ¬è™«
å…ˆæµ‹è¯•å°‘é‡é¡µé¢ç¡®ä¿å·¥ä½œæ­£å¸¸
"""

from expanded_batch_crawler import ExpandedBatchCrawler

class TestExpandedCrawler(ExpandedBatchCrawler):
    """æµ‹è¯•ç‰ˆæ‰©å±•çˆ¬è™«"""
    
    def __init__(self):
        super().__init__()
        self.output_dir = "test_expanded_data"
        
        # åªæµ‹è¯•å‰10ä¸ªé¡µé¢
        self.pages_to_crawl = self.pages_to_crawl[:10]

def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•æ‰©å±•æ‰¹é‡çˆ¬è™« - å‰10é¡µ")
    print("=" * 50)
    
    crawler = TestExpandedCrawler()
    
    try:
        print(f"ğŸš€ æµ‹è¯•çˆ¬å– {len(crawler.pages_to_crawl)} ä¸ªé¡µé¢...")
        
        # çˆ¬å–æ•°æ®
        result = crawler.crawl_all()
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"æ€»é¡µé¢æ•°: {result['total']}")
        print(f"æˆåŠŸçˆ¬å–: {result['success']}")
        print(f"çˆ¬å–å¤±è´¥: {result['failed']}")
        print(f"æˆåŠŸç‡: {result['success']/result['total']*100:.1f}%")
        
        if result['success'] > 0:
            # ç”Ÿæˆå¯¼å…¥æ•°æ®
            import_file = crawler.generate_import_data()
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼ç”Ÿæˆæ–‡ä»¶: {import_file}")
            print(f"ğŸš€ å¯ä»¥è¿è¡Œå®Œæ•´ç‰ˆ: python3 expanded_batch_crawler.py")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
