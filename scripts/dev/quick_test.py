#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬
ç›´æ¥æµ‹è¯•ä¸€ä¸ªURLæ˜¯å¦èƒ½æ­£å¸¸çˆ¬å–å†…å®¹
"""

import requests
from bs4 import BeautifulSoup

def test_single_url():
    """æµ‹è¯•å•ä¸ªURL"""
    # ä½¿ç”¨ä¹‹å‰æµ‹è¯•æˆåŠŸçš„URL
    url = "https://harrypotter.fandom.com/zh/wiki/%E5%88%86%E9%99%A2%E5%B8%BD"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    try:
        print(f"ğŸ” æµ‹è¯•URL: {url}")
        response = requests.get(url, headers=headers, timeout=10)
        print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(response.content)}")
        print(f"ğŸ”£ ç¼–ç : {response.encoding}")
        
        # å¼ºåˆ¶è®¾ç½®UTF-8ç¼–ç 
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æµ‹è¯•æ ‡é¢˜æå–
        title = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
        print(f"ğŸ“ æ ‡é¢˜: {title.get_text().strip() if title else 'æœªæ‰¾åˆ°'}")
        
        # æµ‹è¯•å†…å®¹åŒºåŸŸ
        selectors = [
            ('div#mw-content-text', 'MWå†…å®¹æ–‡æœ¬'),
            ('div.page-content', 'é¡µé¢å†…å®¹'),
            ('div#content', 'å†…å®¹åŒºåŸŸ'),
            ('div.mw-body-content', 'MWä¸»ä½“å†…å®¹'),
        ]
        
        for selector, desc in selectors:
            elem = soup.select_one(selector)
            if elem:
                print(f"âœ… æ‰¾åˆ° {desc}: {len(elem.get_text())} å­—ç¬¦")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {desc}")
        
        # æµ‹è¯•æ®µè½æå–
        paragraphs = soup.find_all('p')
        print(f"ğŸ“ æ®µè½æ•°é‡: {len(paragraphs)}")
        
        valid_paragraphs = []
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 20:
                valid_paragraphs.append(text)
        
        print(f"ğŸ“ æœ‰æ•ˆæ®µè½: {len(valid_paragraphs)}")
        
        if valid_paragraphs:
            print(f"ğŸ“„ ç¬¬ä¸€ä¸ªæ®µè½: {valid_paragraphs[0][:100]}...")
            return True
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ®µè½")
            return False
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ğŸ§ª å¿«é€ŸURLæµ‹è¯•")
    print("=" * 40)
    test_single_url()
