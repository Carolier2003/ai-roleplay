#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ¬è™«æµ‹è¯•è„šæœ¬
å¿«é€Ÿæµ‹è¯•çˆ¬è™«çš„åŸºæœ¬åŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup
import time

def test_wiki_access():
    """æµ‹è¯•ç»´åŸºç½‘ç«™è®¿é—®"""
    print("ğŸ” æµ‹è¯•å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºè®¿é—®...")
    
    try:
        url = "https://harrypotter.fandom.com/zh/wiki/%E5%93%88%E5%88%A9%C2%B7%E6%B3%A2%E7%89%B9"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('h1', {'class': 'page-header__title'})
        
        if title:
            print(f"âœ… è®¿é—®æˆåŠŸï¼é¡µé¢æ ‡é¢˜: {title.get_text().strip()}")
            return True
        else:
            print("âš ï¸ é¡µé¢è®¿é—®æˆåŠŸï¼Œä½†æ ‡é¢˜è§£æå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ è®¿é—®å¤±è´¥: {e}")
        return False

def test_content_extraction():
    """æµ‹è¯•å†…å®¹æå–"""
    print("\nğŸ“„ æµ‹è¯•å†…å®¹æå–...")
    
    try:
        url = "https://harrypotter.fandom.com/zh/wiki/%E5%88%86%E9%99%A2%E5%B8%BD"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
        title_text = title.get_text().strip() if title else "æœªçŸ¥æ ‡é¢˜"
        
        # æå–å†…å®¹ (ä¿®æ­£é€‰æ‹©å™¨)
        content_div = (soup.find('div', {'id': 'mw-content-text'}) or 
                      soup.find('div', {'class': 'page-content'}) or 
                      soup.find('div', {'id': 'content'}))
        if content_div:
            paragraphs = content_div.find_all('p')
            content_preview = ""
            for p in paragraphs[:3]:  # åªå–å‰3æ®µ
                text = p.get_text().strip()
                if len(text) > 20:
                    content_preview += text + "\n\n"
            
            print(f"âœ… å†…å®¹æå–æˆåŠŸï¼")
            print(f"ğŸ“ æ ‡é¢˜: {title_text}")
            print(f"ğŸ“„ å†…å®¹é¢„è§ˆ (å‰3æ®µ):")
            print("-" * 50)
            print(content_preview[:300] + "..." if len(content_preview) > 300 else content_preview)
            print("-" * 50)
            return True
        else:
            print("âŒ å†…å®¹åŒºåŸŸæœªæ‰¾åˆ°")
            return False
            
    except Exception as e:
        print(f"âŒ å†…å®¹æå–å¤±è´¥: {e}")
        return False

def test_multiple_pages():
    """æµ‹è¯•å¤šä¸ªé¡µé¢è®¿é—®"""
    print("\nğŸ”— æµ‹è¯•å¤šä¸ªé¡µé¢è®¿é—®...")
    
    test_urls = [
        "/wiki/%E5%93%88%E5%88%A9%C2%B7%E6%B3%A2%E7%89%B9",  # å“ˆåˆ©Â·æ³¢ç‰¹
        "/wiki/%E8%B5%AB%E6%95%8F%C2%B7%E6%A0%BC%E5%85%B0%E6%9D%B0",  # èµ«æ•Â·æ ¼å…°æ°
        "/wiki/%E9%9C%8D%E6%A0%BC%E6%B2%83%E8%8C%A8%E9%AD%94%E6%B3%95%E5%AD%A6%E6%A0%A1"  # éœæ ¼æ²ƒèŒ¨é­”æ³•å­¦æ ¡
    ]
    
    base_url = "https://harrypotter.fandom.com/zh"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
    }
    
    success_count = 0
    
    for url_path in test_urls:
        try:
            full_url = base_url + url_path
            print(f"  ğŸ” æµ‹è¯•: {url_path}")
            
            response = requests.get(full_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('h1', {'class': 'page-header__title'}) or soup.find('h1')
            
            if title:
                title_text = title.get_text().strip()
                print(f"    âœ… æˆåŠŸ: {title_text}")
                success_count += 1
            else:
                print(f"    âš ï¸ æ ‡é¢˜è§£æå¤±è´¥")
            
            # ç¤¼è²Œæ€§å»¶è¿Ÿ
            time.sleep(1)
            
        except Exception as e:
            print(f"    âŒ å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š å¤šé¡µé¢æµ‹è¯•ç»“æœ: {success_count}/{len(test_urls)} æˆåŠŸ")
    return success_count == len(test_urls)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§™â€â™‚ï¸ å“ˆåˆ©Â·æ³¢ç‰¹ç»´åŸºçˆ¬è™«æµ‹è¯•")
    print("=" * 50)
    
    all_passed = True
    
    # æµ‹è¯•1: åŸºç¡€è®¿é—®
    if not test_wiki_access():
        all_passed = False
    
    # æµ‹è¯•2: å†…å®¹æå–
    if not test_content_extraction():
        all_passed = False
    
    # æµ‹è¯•3: å¤šé¡µé¢è®¿é—®
    if not test_multiple_pages():
        all_passed = False
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬è™«å‡†å¤‡å°±ç»ª")
        print("ğŸ‘‰ è¿è¡Œå‘½ä»¤: ./run_crawler.sh all")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç­‰å¾…é‡è¯•")
    
    return all_passed

if __name__ == "__main__":
    main()
