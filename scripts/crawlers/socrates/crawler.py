#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹æ ¼æ‹‰åº•ç»´åŸºç™¾ç§‘çŸ¥è¯†çˆ¬è™«
ä»ä¸­æ–‡ç»´åŸºç™¾ç§‘çˆ¬å–è‹æ ¼æ‹‰åº•ç›¸å…³çŸ¥è¯†
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from typing import List, Dict
import re

class SocratesCrawler:
    """è‹æ ¼æ‹‰åº•çŸ¥è¯†çˆ¬è™«"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.base_url = "https://zh.wikipedia.org"
        self.output_dir = "../../data/socrates"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è‹æ ¼æ‹‰åº•ç›¸å…³é¡µé¢
        self.pages_to_crawl = [
            {
                "url": "/wiki/%E8%8B%8F%E6%A0%BC%E6%8B%89%E5%BA%95",
                "title": "è‹æ ¼æ‹‰åº•",
                "category": "BIOGRAPHY",
                "importance": 10
            },
            {
                "url": "/wiki/%E8%8B%8F%E6%A0%BC%E6%8B%89%E5%BA%95%E6%95%99%E5%AD%A6%E6%B3%95",
                "title": "è‹æ ¼æ‹‰åº•æ•™å­¦æ³•",
                "category": "PHILOSOPHY",
                "importance": 9
            },
            {
                "url": "/wiki/%E5%8F%A4%E5%B8%8C%E8%85%8A%E5%93%B2%E5%AD%A6",
                "title": "å¤å¸Œè…Šå“²å­¦",
                "category": "PHILOSOPHY",
                "importance": 8
            },
            {
                "url": "/wiki/%E6%9F%8F%E6%8B%89%E5%9B%BE",
                "title": "æŸæ‹‰å›¾",
                "category": "STUDENT",
                "importance": 9
            },
            {
                "url": "/wiki/%E9%9B%85%E5%85%B8",
                "title": "é›…å…¸",
                "category": "LOCATION",
                "importance": 8
            },
            {
                "url": "/wiki/%E7%9F%A5%E8%AF%86%E8%AE%BA",
                "title": "çŸ¥è¯†è®º",
                "category": "PHILOSOPHY",
                "importance": 7
            },
            {
                "url": "/wiki/%E4%BC%A6%E7%90%86%E5%AD%A6",
                "title": "ä¼¦ç†å­¦",
                "category": "PHILOSOPHY",
                "importance": 7
            },
            {
                "url": "/wiki/%E5%BE%B7%E6%80%A7%E4%BC%A6%E7%90%86%E5%AD%A6",
                "title": "å¾·æ€§ä¼¦ç†å­¦",
                "category": "PHILOSOPHY",
                "importance": 8
            }
        ]
    
    def get_page_content(self, url: str) -> BeautifulSoup:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            full_url = self.base_url + url
            print(f"ğŸ” æ­£åœ¨è·å–: {full_url}")
            
            response = requests.get(full_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup
            
        except requests.RequestException as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None
    
    def extract_page_data(self, soup: BeautifulSoup, page_info: Dict) -> Dict:
        """ä»é¡µé¢æå–æ•°æ®"""
        if not soup:
            return None
            
        try:
            # æå–æ ‡é¢˜
            title = page_info["title"]
            h1 = soup.find('h1', {'class': 'firstHeading'})
            if h1:
                title = h1.get_text().strip()
            
            # ç»´åŸºç™¾ç§‘çš„ä¸»è¦å†…å®¹åŒºåŸŸ
            content_div = None
            content_selectors = [
                'div#mw-content-text',
                'div.mw-parser-output', 
                'div#content',
                'div.mw-body-content'
            ]
            
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    break
            
            if not content_div:
                print(f"âŒ æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸ")
                return None
            
            # æ¸…ç†å’Œæå–å†…å®¹
            content = self.clean_wikipedia_content(content_div)
            
            if len(content) < 100:
                print(f"âŒ å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æå–å¤±è´¥")
                return None
            
            # æå–ä¿¡æ¯æ¡†æ•°æ®
            infobox_data = self.extract_infobox(soup)
            
            # æå–åˆ†ç±»ä¿¡æ¯
            categories = self.extract_categories(soup)
            
            return {
                "title": title,
                "content": content,
                "category": page_info["category"],
                "importance": page_info["importance"],
                "url": page_info["url"],
                "infobox": infobox_data,
                "categories": categories,
                "character_id": 2,  # è‹æ ¼æ‹‰åº•çš„è§’è‰²ID
                "source": "ä¸­æ–‡ç»´åŸºç™¾ç§‘"
            }
            
        except Exception as e:
            print(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
            return None
    
    def clean_wikipedia_content(self, content_div: BeautifulSoup) -> str:
        """æ¸…ç†ç»´åŸºç™¾ç§‘å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in content_div.find_all(['script', 'style', 'table.navbox', 'div.navbox', 
                                       'div.reflist', 'div.printfooter', 'div.catlinks']):
            tag.decompose()
        
        # ç§»é™¤å¼•ç”¨æ ‡è®°
        for tag in content_div.find_all('sup', class_='reference'):
            tag.decompose()
        
        # ç§»é™¤å›¾ç‰‡è¯´æ˜ç­‰
        for tag in content_div.find_all('div', class_=['thumbcaption', 'magnify']):
            tag.decompose()
        
        # æå–æ®µè½æ–‡æœ¬
        paragraphs = []
        for p in content_div.find_all('p'):
            text = p.get_text().strip()
            if text and len(text) > 20:
                # æ¸…ç†æ–‡æœ¬
                text = re.sub(r'\[\d+\]', '', text)  # ç§»é™¤å¼•ç”¨æ ‡è®°
                text = re.sub(r'\s+', ' ', text)     # æ ‡å‡†åŒ–ç©ºç™½ç¬¦
                paragraphs.append(text)
        
        return '\n\n'.join(paragraphs)
    
    def extract_infobox(self, soup: BeautifulSoup) -> Dict:
        """æå–ä¿¡æ¯æ¡†æ•°æ®"""
        infobox = {}
        infobox_table = soup.find('table', class_='infobox')
        
        if infobox_table:
            for row in infobox_table.find_all('tr'):
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text().strip()
                    value = td.get_text().strip()
                    if key and value:
                        infobox[key] = value
        
        return infobox
    
    def extract_categories(self, soup: BeautifulSoup) -> List[str]:
        """æå–é¡µé¢åˆ†ç±»"""
        categories = []
        catlinks = soup.find('div', id='catlinks')
        if catlinks:
            for link in catlinks.find_all('a'):
                if '/wiki/Category:' in link.get('href', ''):
                    categories.append(link.get_text().strip())
        
        return categories
    
    def make_safe_filename(self, filename: str) -> str:
        """åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.replace(' ', '_')
        return filename[:100]  # é™åˆ¶é•¿åº¦
    
    def save_data(self, data: Dict, filename: str):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ å·²ä¿å­˜: {filepath}")
    
    def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        print("ğŸš€ å¼€å§‹çˆ¬å–è‹æ ¼æ‹‰åº•ç›¸å…³çŸ¥è¯†...")
        print(f"ğŸ“Š æ€»é¡µé¢æ•°: {len(self.pages_to_crawl)}")
        
        all_data = []
        success_count = 0
        
        for i, page_info in enumerate(self.pages_to_crawl, 1):
            print(f"\nğŸ“– å¤„ç†ç¬¬ {i}/{len(self.pages_to_crawl)} é¡µ: {page_info['title']}")
            
            try:
                # è·å–é¡µé¢å†…å®¹
                soup = self.get_page_content(page_info["url"])
                if not soup:
                    continue
                
                # æå–æ•°æ®
                data = self.extract_page_data(soup, page_info)
                if not data:
                    continue
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶
                safe_filename = self.make_safe_filename(page_info["title"])
                self.save_data(data, f"{safe_filename}.json")
                
                all_data.append(data)
                success_count += 1
                
                print(f"âœ… æˆåŠŸçˆ¬å–: {page_info['title']}")
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(data['content'])} å­—ç¬¦")
                
                # å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {page_info['title']}, é”™è¯¯: {e}")
                continue
        
        # ä¿å­˜åˆå¹¶æ•°æ®
        if all_data:
            self.save_data(all_data, "socrates_knowledge_base.json")
            
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self.generate_report(all_data, success_count)
    
    def generate_report(self, all_data: List[Dict], success_count: int):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        total_pages = len(self.pages_to_crawl)
        total_content = sum(len(data['content']) for data in all_data)
        
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "character": "è‹æ ¼æ‹‰åº•",
            "total_pages": total_pages,
            "successful_pages": success_count,
            "success_rate": f"{success_count/total_pages*100:.1f}%",
            "total_content_length": total_content,
            "avg_content_length": int(total_content/success_count) if success_count > 0 else 0,
            "categories": list(set(data['category'] for data in all_data)),
            "pages": [{"title": data['title'], "length": len(data['content'])} for data in all_data]
        }
        
        self.save_data(report, "crawl_report.json")
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»é¡µé¢æ•°: {total_pages}")
        print(f"   - æˆåŠŸçˆ¬å–: {success_count}")
        print(f"   - æˆåŠŸç‡: {report['success_rate']}")
        print(f"   - æ€»å†…å®¹: {total_content:,} å­—ç¬¦")
        print(f"   - å¹³å‡é•¿åº¦: {report['avg_content_length']:,} å­—ç¬¦")

if __name__ == "__main__":
    crawler = SocratesCrawler()
    crawler.crawl_all()