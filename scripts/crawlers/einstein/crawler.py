#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çˆ±å› æ–¯å¦ç»´åŸºç™¾ç§‘çˆ¬è™«
çˆ¬å–çˆ±å› æ–¯å¦åŠå…¶ç›¸å…³ç§‘å­¦æ¦‚å¿µçš„çŸ¥è¯†
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from typing import List, Dict, Any
import logging
import urllib.parse

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EinsteinCrawler:
    """çˆ±å› æ–¯å¦ç»´åŸºç™¾ç§‘çˆ¬è™«"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.base_url = "https://zh.wikipedia.org"
        self.output_dir = "../../data/einstein"  # ç»Ÿä¸€æ•°æ®å­˜å‚¨ä½ç½®
        os.makedirs(self.output_dir, exist_ok=True)
        self.character_name = "çˆ±å› æ–¯å¦"

        # è¦çˆ¬å–çš„é¡µé¢åˆ—è¡¨ - åŸºäºçˆ±å› æ–¯å¦çš„ç§‘å­¦è´¡çŒ®å’Œç›¸å…³æ¦‚å¿µ
        self.pages_to_crawl = [
            # æ ¸å¿ƒä¼ è®°
            {"url": "/wiki/%E9%98%BF%E5%B0%94%E4%BC%AF%E7%89%B9%C2%B7%E7%88%B1%E5%9B%A0%E6%96%AF%E5%9D%A6", "title": "é˜¿å°”ä¼¯ç‰¹Â·çˆ±å› æ–¯å¦", "category": "BIOGRAPHY", "importance": 10},
            
            # ç›¸å¯¹è®ºç†è®º
            {"url": "/wiki/%E7%9B%B8%E5%AF%B9%E8%AE%BA", "title": "ç›¸å¯¹è®º", "category": "PHYSICS_THEORY", "importance": 10},
            {"url": "/wiki/%E7%8B%AD%E4%B9%89%E7%9B%B8%E5%AF%B9%E8%AE%BA", "title": "ç‹­ä¹‰ç›¸å¯¹è®º", "category": "PHYSICS_THEORY", "importance": 9},
            {"url": "/wiki/%E5%B9%BF%E4%B9%89%E7%9B%B8%E5%AF%B9%E8%AE%BA", "title": "å¹¿ä¹‰ç›¸å¯¹è®º", "category": "PHYSICS_THEORY", "importance": 9},
            
            # è´¨èƒ½æ–¹ç¨‹å’Œé‡è¦å‘ç°
            {"url": "/wiki/%E8%B4%A8%E8%83%BD%E6%96%B9%E7%A8%8B", "title": "è´¨èƒ½æ–¹ç¨‹", "category": "PHYSICS_FORMULA", "importance": 9},
            {"url": "/wiki/%E5%85%89%E7%94%B5%E6%95%88%E5%BA%94", "title": "å…‰ç”µæ•ˆåº”", "category": "PHYSICS_PHENOMENON", "importance": 8},
            {"url": "/wiki/%E5%B8%83%E6%9C%97%E8%BF%90%E5%8A%A8", "title": "å¸ƒæœ—è¿åŠ¨", "category": "PHYSICS_PHENOMENON", "importance": 7},
            
            # é‡å­åŠ›å­¦ç›¸å…³
            {"url": "/wiki/%E9%87%8F%E5%AD%90%E5%8A%9B%E5%AD%A6", "title": "é‡å­åŠ›å­¦", "category": "PHYSICS_THEORY", "importance": 8},
            {"url": "/wiki/%E5%85%89%E5%AD%90", "title": "å…‰å­", "category": "PHYSICS_CONCEPT", "importance": 7},
            {"url": "/wiki/%E6%B3%A2%E7%B2%92%E4%BA%8C%E8%B1%A1%E6%80%A7", "title": "æ³¢ç²’äºŒè±¡æ€§", "category": "PHYSICS_CONCEPT", "importance": 7},
            
            # ç§‘å­¦æœºæ„å’Œå†å²èƒŒæ™¯
            {"url": "/wiki/%E6%99%AE%E6%9E%97%E6%96%AF%E9%A1%BF%E9%AB%98%E7%AD%89%E7%A0%94%E7%A9%B6%E9%99%A2", "title": "æ™®æ—æ–¯é¡¿é«˜ç­‰ç ”ç©¶é™¢", "category": "INSTITUTION", "importance": 6},
            {"url": "/wiki/%E7%91%9E%E5%A3%AB%E8%81%94%E9%82%A6%E7%90%86%E5%B7%A5%E5%AD%A6%E9%99%A2", "title": "ç‘å£«è”é‚¦ç†å·¥å­¦é™¢", "category": "INSTITUTION", "importance": 6},
            
            # è¯ºè´å°”å¥–å’Œè£èª‰
            {"url": "/wiki/1921%E5%B9%B4%E8%AF%BA%E8%B4%9D%E5%B0%94%E7%89%A9%E7%90%86%E5%AD%A6%E5%A5%96", "title": "1921å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–", "category": "AWARD", "importance": 8},
            
            # åŒæ—¶ä»£çš„ç§‘å­¦å®¶
            {"url": "/wiki/%E5%B0%BC%E5%B0%94%E6%96%AF%C2%B7%E7%8E%BB%E5%B0%94", "title": "å°¼å°”æ–¯Â·ç»å°”", "category": "SCIENTIST", "importance": 7},
            {"url": "/wiki/%E9%A9%AC%E5%85%8B%E6%96%AF%C2%B7%E6%99%AE%E6%9C%97%E5%85%8B", "title": "é©¬å…‹æ–¯Â·æ™®æœ—å…‹", "category": "SCIENTIST", "importance": 7},
            
            # ç§‘å­¦å“²å­¦å’Œæ€æƒ³
            {"url": "/wiki/%E7%A7%91%E5%AD%A6%E5%93%B2%E5%AD%A6", "title": "ç§‘å­¦å“²å­¦", "category": "PHILOSOPHY", "importance": 6},
            {"url": "/wiki/%E7%BB%9F%E4%B8%80%E5%9C%BA%E8%AE%BA", "title": "ç»Ÿä¸€åœºè®º", "category": "PHYSICS_THEORY", "importance": 7},
        ]
        
        self.crawled_data = []
        self.successful_pages = 0
        self.failed_pages = 0
        self.total_content_length = 0

    def get_page_content(self, url: str) -> BeautifulSoup:
        """è·å–é¡µé¢å†…å®¹å¹¶è§£æä¸ºBeautifulSoupå¯¹è±¡"""
        full_url = f"{self.base_url}{url}"
        try:
            response = requests.get(full_url, headers=self.headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ— æ³•è·å–é¡µé¢ {full_url}: {e}")
            return None

    def extract_page_data(self, soup: BeautifulSoup, url: str, title: str, category: str, importance: int) -> Dict[str, Any]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–é¡µé¢æ•°æ®"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
        content_div = None
        selectors = [
            'div.mw-content-ltr.mw-parser-output',  # æœ€å¸¸è§çš„ç»´åŸºç™¾ç§‘ç»“æ„
            'div.mw-parser-output',
            'div#mw-content-text .mw-parser-output',
            'div#mw-content-text'
        ]
        
        for selector in selectors:
            content_div = soup.select_one(selector)
            if content_div:
                paragraphs = content_div.find_all('p')
                if paragraphs:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                    break
        
        if not content_div:
            logger.warning(f"âŒ å†…å®¹åŒºåŸŸæœªæ‰¾åˆ°: {title} ({url})")
            return None

        # ç§»é™¤ä¸ç›¸å…³å…ƒç´ 
        for selector in ['table.infobox', 'div.toc', 'div.reflist', 'div.navbox', 'div.mw-references-wrap',
                         'div.printfooter', 'div.mw-indicator', 'div.catlinks', 'div.sister-project',
                         'div.ambox', 'div.metadata', 'span.mw-editsection', 'sup.reference', 'ol.references',
                         'table.wikitable', 'div.thumb', 'div.gallery']:
            for item in content_div.find_all(selector):
                item.decompose()

        # æå–ä¸»è¦æ®µè½æ–‡æœ¬
        paragraphs = content_div.find_all('p', recursive=False)
        content_text = "\n".join([p.get_text(separator=" ", strip=True) for p in paragraphs if p.get_text(strip=True)])

        # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•æ›´å¹¿èŒƒå›´çš„æå–
        if len(content_text) < 150:
            all_text_elements = content_div.find_all(['p', 'h2', 'h3', 'h4', 'li'])
            content_text = "\n".join([elem.get_text(separator=" ", strip=True) for elem in all_text_elements if elem.get_text(strip=True)])

        content_text = self.clean_content(content_text)

        if not content_text or len(content_text) < 100:
            logger.warning(f"âŒ æå–å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­: {title} ({url})")
            return None

        # æå–ä¿¡æ¯æ¡†æ•°æ®
        infobox = {}
        info_table = soup.find('table', class_='infobox')
        if info_table:
            for row in info_table.find_all('tr'):
                header = row.find('th')
                data = row.find('td')
                if header and data:
                    key = header.get_text(strip=True)
                    value = data.get_text(strip=True)
                    if key and value:
                        infobox[key] = value

        # æå–æ ‡ç­¾
        tags = []
        # æ ¹æ®åˆ†ç±»æ·»åŠ æ ‡ç­¾
        category_tags = {
            "BIOGRAPHY": ["ä¼ è®°", "ç§‘å­¦å®¶", "ç‰©ç†å­¦å®¶"],
            "PHYSICS_THEORY": ["ç‰©ç†ç†è®º", "ç†è®ºç‰©ç†", "ç§‘å­¦"],
            "PHYSICS_FORMULA": ["ç‰©ç†å…¬å¼", "æ•°å­¦", "ç§‘å­¦"],
            "PHYSICS_PHENOMENON": ["ç‰©ç†ç°è±¡", "å®éªŒç‰©ç†", "ç§‘å­¦"],
            "PHYSICS_CONCEPT": ["ç‰©ç†æ¦‚å¿µ", "ç†è®º", "ç§‘å­¦"],
            "INSTITUTION": ["ç§‘å­¦æœºæ„", "æ•™è‚²", "å­¦æœ¯"],
            "AWARD": ["ç§‘å­¦å¥–é¡¹", "è£èª‰", "è¯ºè´å°”å¥–"],
            "SCIENTIST": ["ç§‘å­¦å®¶", "ç‰©ç†å­¦å®¶", "åŒæ—¶ä»£äºº"],
            "PHILOSOPHY": ["ç§‘å­¦å“²å­¦", "å“²å­¦", "æ€æƒ³"]
        }
        
        tags.extend(category_tags.get(category, [category]))
        tags.extend([self.character_name, "20ä¸–çºªç§‘å­¦", "ç°ä»£ç‰©ç†"])

        return {
            "characterId": 3,  # çˆ±å› æ–¯å¦çš„è§’è‰²ID
            "title": title,
            "content": content_text,
            "knowledgeType": category,
            "importanceScore": importance,
            "source": "ä¸­æ–‡ç»´åŸºç™¾ç§‘",
            "sourceUrl": f"{self.base_url}{url}",
            "tags": tags,
            "language": "zh"
        }

    def clean_content(self, content: str) -> str:
        """æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
        content = ' '.join(content.split())
        # ç§»é™¤ç»´åŸºç™¾ç§‘ç‰¹æœ‰çš„ç¼–è¾‘æç¤º
        content = content.replace('[ç¼–è¾‘]', '').replace('[æŸ¥]', '').replace('[è®º]', '').replace('[é˜…]', '')
        # ç§»é™¤å¼•ç”¨æ ‡è®°
        content = self.remove_citations(content)
        return content.strip()

    def remove_citations(self, text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„å¼•ç”¨æ ‡è®°ï¼Œå¦‚ [1], [2]"""
        import re
        return re.sub(r'\[\d+\]', '', text)

    def make_safe_filename(self, title: str) -> str:
        """å°†æ ‡é¢˜è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '.', '_', '-')]).rstrip()
        return safe_title.replace(' ', '_')

    def crawl_single_page(self, page_info: Dict[str, Any]):
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        url = page_info["url"]
        title = page_info["title"]
        category = page_info["category"]
        importance = page_info["importance"]

        logger.info(f"ğŸ“– å¤„ç†ç¬¬ {self.successful_pages + self.failed_pages + 1}/{len(self.pages_to_crawl)} é¡µ: {title}")
        logger.info(f"ğŸ” æ­£åœ¨è·å–: {self.base_url}{url}")

        soup = self.get_page_content(url)
        if soup:
            data = self.extract_page_data(soup, url, title, category, importance)
            if data and len(data["content"]) > 100:
                filename = f"{self.make_safe_filename(title)}.json"
                filepath = os.path.join(self.output_dir, filename)
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                self.crawled_data.append(data)
                self.successful_pages += 1
                self.total_content_length += len(data["content"])
                logger.info(f"ğŸ’¾ å·²ä¿å­˜: {filepath}")
                logger.info(f"âœ… æˆåŠŸçˆ¬å–: {title}")
                logger.info(f"ğŸ“ å†…å®¹é•¿åº¦: {len(data['content'])} å­—ç¬¦")
            else:
                self.failed_pages += 1
                logger.warning(f"âŒ çˆ¬å–å¤±è´¥æˆ–å†…å®¹è¿‡çŸ­: {title}")
        else:
            self.failed_pages += 1
            logger.error(f"âŒ æ— æ³•è·å–æˆ–è§£æé¡µé¢: {title}")
        
        # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1.5)

    def crawl_all(self):
        """åè°ƒæ•´ä¸ªçˆ¬å–è¿‡ç¨‹"""
        logger.info("ğŸš€ å¼€å§‹çˆ¬å–çˆ±å› æ–¯å¦ç›¸å…³çŸ¥è¯†...")
        logger.info(f"ğŸ“Š æ€»é¡µé¢æ•°: {len(self.pages_to_crawl)}")

        for page_info in self.pages_to_crawl:
            self.crawl_single_page(page_info)

        # ä¿å­˜æ‰€æœ‰çˆ¬å–åˆ°çš„æ•°æ®åˆ°ä¸€ä¸ªæ€»æ–‡ä»¶
        consolidated_filename = os.path.join(self.output_dir, f"{self.character_name.lower().replace(' ', '_')}_knowledge_base.json")
        with open(consolidated_filename, 'w', encoding='utf-8') as f:
            json.dump(self.crawled_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜: {consolidated_filename}")

        # ç”Ÿæˆçˆ¬å–æŠ¥å‘Š
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "character": self.character_name,
            "total_pages": len(self.pages_to_crawl),
            "successful_pages": self.successful_pages,
            "failed_pages": self.failed_pages,
            "success_rate": f"{self.successful_pages / len(self.pages_to_crawl) * 100:.1f}%",
            "total_content_length": self.total_content_length,
            "avg_content_length": self.total_content_length / self.successful_pages if self.successful_pages > 0 else 0,
            "categories": list(set([p["category"] for p in self.pages_to_crawl])),
            "pages": [{"title": d["title"], "length": len(d["content"]), "category": d["knowledgeType"]} for d in self.crawled_data]
        }
        report_filename = os.path.join(self.output_dir, "crawl_report.json")
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜: {report_filename}")

        logger.info("\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   - æ€»é¡µé¢æ•°: {len(self.pages_to_crawl)}")
        logger.info(f"   - æˆåŠŸçˆ¬å–: {self.successful_pages}")
        logger.info(f"   - å¤±è´¥é¡µé¢: {self.failed_pages}")
        logger.info(f"   - æˆåŠŸç‡: {report['success_rate']}")
        logger.info(f"   - æ€»å†…å®¹: {self.total_content_length} å­—ç¬¦")
        logger.info(f"   - å¹³å‡é•¿åº¦: {report['avg_content_length']:.0f} å­—ç¬¦")
        logger.info(f"   - çŸ¥è¯†åˆ†ç±»: {', '.join(report['categories'])}")

if __name__ == "__main__":
    crawler = EinsteinCrawler()
    crawler.crawl_all()