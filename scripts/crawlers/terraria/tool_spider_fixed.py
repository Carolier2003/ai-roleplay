#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ³°æ‹‰ç‘äºšå·¥å…·ä¿¡æ¯çˆ¬è™« - ä¿®å¤ç‰ˆ
å®Œå…¨å¤åˆ¶ç‹¬ç«‹æµ‹è¯•è„šæœ¬çš„æˆåŠŸé€»è¾‘
"""

import requests
import time
import random
import re
import json
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
from cf_bypass import cf_make_request

class TerrariaToolSpiderFixed:
    """æ³°æ‹‰ç‘äºšå·¥å…·ä¿¡æ¯çˆ¬è™« - ä¿®å¤ç‰ˆ"""
    
    def __init__(self):
        # å®Œå…¨å¤åˆ¶ç‹¬ç«‹æµ‹è¯•è„šæœ¬çš„æˆåŠŸé…ç½®
        self.base_url = "https://terraria.wiki.gg"
        self.main_url = "https://terraria.wiki.gg/zh/wiki/å·¥å…·"
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.tool_links = []
        self.tools_data = []
        self.failed_urls = []
        
    def make_request(self, url, max_retries=5):
        """æ”¹è¿›çš„è¯·æ±‚æ–¹æ³•ï¼Œç¡®ä¿è·å–å®Œæ•´é¡µé¢"""
        print(f"ğŸŒ è¯·æ±‚é¡µé¢: {url}")
        
        # é’ˆå¯¹å·¥å…·é¡µé¢ï¼ŒæœŸæœ›çš„å®Œæ•´é¡µé¢å¤§å°ï¼ˆæ ¹æ®è°ƒè¯•ç»“æœè°ƒæ•´ï¼‰
        min_expected_size = 80000  # é™ä½æœŸæœ›ï¼Œå¤§å¤šæ•°å·¥å…·é¡µé¢å¯èƒ½æ²¡æœ‰300K
        
        for attempt in range(max_retries):
            try:
                # æ¯æ¬¡é‡è¯•ä½¿ç”¨æ–°çš„session
                if attempt > 0:
                    self.session = requests.Session()
                    print(f"ğŸ”„ ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼ˆæ–°sessionï¼‰...")
                    time.sleep(random.uniform(2, 4))  # å¢åŠ å»¶æ—¶
                
                response = cf_make_request(url, self.headers, self.session, 30, 3)
                
                if response and response.status_code == 200:
                    print(f"âœ… CFç»•è¿‡æˆåŠŸï¼Œå¤§å°: {len(response.content)} bytes")
                    
                    try:
                        html_content = response.text
                        print(f"ğŸ“ HTMLå†…å®¹å¤§å°: {len(html_content)} å­—ç¬¦")
                        
                        # å¤šç§éªŒè¯æ–¹æ³•
                        is_complete = False
                        
                        # æ–¹æ³•1ï¼šå¤§å°éªŒè¯ï¼ˆä¸»è¦å·¥å…·é¡µé¢åº”è¯¥è¾ƒå¤§ï¼‰
                        if len(html_content) > min_expected_size:
                            is_complete = True
                            print(f"âœ… å¤§å°éªŒè¯é€šè¿‡")
                        
                        # æ–¹æ³•2ï¼šå…³é”®æ ‡ç­¾éªŒè¯
                        if '<h1' in html_content and '</html>' in html_content:
                            if not is_complete and len(html_content) > 20000:
                                is_complete = True
                                print(f"âœ… ç»“æ„éªŒè¯é€šè¿‡ï¼ˆè¾ƒå°ä½†å®Œæ•´ï¼‰")
                        
                        # æ–¹æ³•3ï¼šå†…å®¹è´¨é‡éªŒè¯
                        if 'firstHeading' in html_content or 'mw-page-title-main' in html_content:
                            if not is_complete and len(html_content) > 15000:
                                is_complete = True
                                print(f"âœ… å†…å®¹éªŒè¯é€šè¿‡ï¼ˆåŒ…å«æ ‡é¢˜ï¼‰")
                        
                        if is_complete:
                            return html_content
                        else:
                            print(f"âš ï¸ é¡µé¢å†…å®¹ä¸å®Œæ•´ï¼Œé‡è¯•...")
                            
                    except Exception as e:
                        print(f"âŒ HTMLå¤„ç†å¤±è´¥: {e}")
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code if response else 'None'}")
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        
        print(f"âŒ {max_retries}æ¬¡å°è¯•å‡å¤±è´¥")
        return None
    
    def extract_tool_links(self, html_content):
        """æå–å·¥å…·é“¾æ¥"""
        print("ğŸ” æå–å·¥å…·é“¾æ¥...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰ itemlist terraria åŒºåŸŸ
        itemlist_divs = soup.find_all('div', class_='itemlist terraria')
        print(f"ğŸ“‹ æ‰¾åˆ° {len(itemlist_divs)} ä¸ªå·¥å…·åˆ—è¡¨åŒºåŸŸ")
        
        tool_links = set()
        for itemlist_div in itemlist_divs:
            li_elements = itemlist_div.find_all('li')
            print(f"  ğŸ“Š å½“å‰åŒºåŸŸæœ‰ {len(li_elements)} ä¸ªå·¥å…·é¡¹")
            
            for li in li_elements:
                # å–ç¬¬ä¸€ä¸ªaæ ‡ç­¾
                a_tag = li.find('a', href=re.compile(r'/zh/wiki/'))
                if a_tag and a_tag.get('href'):
                    href = a_tag.get('href')
                    title = a_tag.get('title', '').strip()
                    
                    # è¡¥å…¨ç»å¯¹è·¯å¾„å¹¶è§£ç URL
                    full_url = urljoin(self.base_url, href)
                    # å¯¹URLè¿›è¡Œè§£ç ä»¥å¤„ç†ä¸­æ–‡å­—ç¬¦
                    decoded_url = unquote(full_url)
                    tool_links.add(decoded_url)
                    
                    if title:
                        print(f"    âœ… å‘ç°å·¥å…·: {title}")
        
        self.tool_links = list(tool_links)
        print(f"ğŸ“Š æ€»è®¡å‘ç° {len(self.tool_links)} ä¸ªä¸é‡å¤çš„å·¥å…·é“¾æ¥")
        return self.tool_links
    
    def run_crawler(self):
        """è¿è¡Œçˆ¬è™«ä¸»æµç¨‹"""
        print("ğŸš€ æ³°æ‹‰ç‘äºšå·¥å…·ä¿¡æ¯çˆ¬è™«å¯åŠ¨ï¼ˆä¿®å¤ç‰ˆï¼‰")
        print("=" * 50)
        
        # 1. è·å–ä¸»é¡µé¢
        main_html = self.make_request(self.main_url)
        if not main_html:
            print("âŒ æ— æ³•è·å–ä¸»é¡µé¢ï¼Œçˆ¬è™«ç»ˆæ­¢")
            self.failed_urls.append(self.main_url)
            self.save_failed_urls()
            return False
        
        # 2. æå–å·¥å…·é“¾æ¥
        if not self.extract_tool_links(main_html):
            print("âŒ æ— æ³•æå–å·¥å…·é“¾æ¥ï¼Œçˆ¬è™«ç»ˆæ­¢")
            self.failed_urls.append(self.main_url)
            self.save_failed_urls()
            return False
        
        # 3. çˆ¬å–æ‰€æœ‰å·¥å…·
        print(f"\nğŸš€ å¼€å§‹çˆ¬å–å…¨éƒ¨ {len(self.tool_links)} ä¸ªå·¥å…·...")
        
        for i, tool_url in enumerate(self.tool_links):
            tool_data = self.crawl_single_tool(tool_url, i+1, len(self.tool_links))
            if tool_data:
                self.tools_data.append(tool_data)
                print(f"    âœ… å·²é‡‡é›† {len(self.tools_data)} ä¸ªå·¥å…·ï¼š{tool_data.get('å·¥å…·å', 'æœªçŸ¥')}")
            else:
                print(f"    âŒ å·¥å…· {i+1} çˆ¬å–å¤±è´¥")
            
            # å»¶æ—¶ï¼ˆé¿å…è¯·æ±‚è¿‡å¿«ï¼‰
            time.sleep(random.uniform(0.3, 0.8))
            
            # æ¯10ä¸ªå·¥å…·æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10 == 0:
                progress = (i + 1) / len(self.tool_links) * 100
                print(f"ğŸ“Š è¿›åº¦: {i+1}/{len(self.tool_links)} ({progress:.1f}%)")
                # ä¿å­˜ä¸­é—´ç»“æœ
                self.save_results()
        
        # 4. ä¿å­˜æµ‹è¯•ç»“æœ
        self.save_results()
        self.save_failed_urls()
        
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print(f"âœ… æˆåŠŸçˆ¬å–: {len(self.tools_data)} ä¸ªå·¥å…·")
        print(f"âŒ å¤±è´¥: {len(self.failed_urls)} ä¸ª")
        
        return True
    
    def crawl_single_tool(self, tool_url, current, total):
        """çˆ¬å–å•ä¸ªå·¥å…·ä¿¡æ¯"""
        print(f"ğŸ”§ [{current}/{total}] çˆ¬å–å·¥å…·: {tool_url}")
        
        # ä½¿ç”¨æ”¹è¿›çš„è¯·æ±‚æ–¹æ³•ï¼ˆå†…ç½®é‡è¯•æœºåˆ¶ï¼‰
        html_content = self.make_request(tool_url, max_retries=5)
        
        if not html_content:
            print(f"    âŒ é¡µé¢è·å–å¤±è´¥")
            self.failed_urls.append(tool_url)
            return None
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # å¤šç§æ–¹æ³•æå–å·¥å…·å
        tool_name = "æœªçŸ¥å·¥å…·"
        
        # æ–¹æ³•1ï¼šh1.firstHeading
        title_tag = soup.find('h1', class_='firstHeading')
        if title_tag:
            tool_name = title_tag.get_text().strip()
        else:
            # æ–¹æ³•2ï¼šä»»ä½•h1æ ‡ç­¾
            title_tag = soup.find('h1')
            if title_tag:
                tool_name = title_tag.get_text().strip()
            else:
                # æ–¹æ³•3ï¼šä»é¡µé¢titleæå–
                title_tag = soup.find('title')
                if title_tag:
                    page_title = title_tag.get_text().strip()
                    if " - " in page_title:
                        tool_name = page_title.split(" - ")[0].strip()
                    else:
                        # æ–¹æ³•4ï¼šä»URLæå–
                        url_parts = tool_url.split('/')
                        if url_parts:
                            from urllib.parse import unquote
                            tool_name = unquote(url_parts[-1]).replace('_', ' ')
        
        print(f"    ğŸ“ å·¥å…·å: {tool_name}")
        
        # æå–åŸºæœ¬å±æ€§
        attributes = self.extract_tool_attributes(soup)
        
        # æå–å·¥å…·èƒ½åŠ›
        tool_power = self.extract_tool_power(soup)
        
        # æå–é…æ–¹
        recipes = self.extract_tool_recipes(soup)
        
        tool_data = {
            "å·¥å…·å": tool_name,
            "å±æ€§": attributes,
            "å·¥å…·èƒ½åŠ›": tool_power,
            "é…æ–¹è¡¨": recipes,
            "url": tool_url
        }
        
        return tool_data
    
    def extract_tool_attributes(self, soup):
        """æå–å·¥å…·å±æ€§"""
        attributes = {}
        
        # æŸ¥æ‰¾statè¡¨æ ¼
        stat_tables = soup.find_all('table', class_='stat')
        for table in stat_tables:
            rows = table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.get_text(strip=True)
                    value = td.get_text(strip=True)
                    if key and value:
                        attributes[key] = value
        
        return attributes
    
    def extract_tool_power(self, soup):
        """æå–å·¥å…·èƒ½åŠ›"""
        tool_power = {}
        
        # æŸ¥æ‰¾toolpowerç›¸å…³ä¿¡æ¯
        toolpower_sections = soup.find_all('div', class_='toolpower')
        for section in toolpower_sections:
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
            pass
        
        return tool_power
    
    def extract_tool_recipes(self, soup):
        """æå–åˆ¶ä½œé…æ–¹"""
        recipes = []
        
        # æŸ¥æ‰¾é…æ–¹è¡¨æ ¼
        recipe_tables = soup.find_all('table', class_=lambda x: x and 'recipes' in ' '.join(x) if x else False)
        for table in recipe_tables:
            tbody = table.find('tbody')
            if tbody:
                rows = tbody.find_all('tr')
                for row in rows:
                    tds = row.find_all('td')
                    if len(tds) >= 3:
                        recipe = {
                            "äº§ç‰©": tds[0].get_text(strip=True),
                            "ææ–™": [li.get_text(strip=True) for li in tds[1].find_all('li')] if tds[1].find_all('li') else [tds[1].get_text(strip=True)],
                            "åˆ¶ä½œç«™": tds[2].get_text(strip=True)
                        }
                        recipes.append(recipe)
        
        return recipes
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        output_file = "../../data/terraria/terraria_tools_fixed.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for tool_data in self.tools_data:
                f.write(json.dumps(tool_data, ensure_ascii=False, separators=(',', ':')) + '\n')
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def save_failed_urls(self):
        """ä¿å­˜å¤±è´¥çš„URL"""
        if self.failed_urls:
            failed_file = "../../data/terraria/failed_fixed.txt"
            with open(failed_file, 'w', encoding='utf-8') as f:
                for url in self.failed_urls:
                    f.write(url + '\n')
            print(f"âš ï¸ ä¿å­˜ {len(self.failed_urls)} ä¸ªå¤±è´¥URLåˆ°: {failed_file}")

def main():
    """ä¸»å‡½æ•°"""
    spider = TerrariaToolSpiderFixed()
    spider.run_crawler()

if __name__ == "__main__":
    main()
