#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±Ÿæˆ·å·æŸ¯å—ï¼ˆåä¾¦æ¢æŸ¯å—ï¼‰çŸ¥è¯†çˆ¬è™«
ä»æŸ¯å—ç™¾ç§‘çˆ¬å–ç›¸å…³çŸ¥è¯†
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from typing import List, Dict
import re
import urllib.parse

class ConanCrawler:
    """æŸ¯å—çŸ¥è¯†çˆ¬è™«"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.base_url = "https://www.conanpedia.com"
        self.output_dir = "../../data/conan"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # âš ï¸ é‡è¦ï¼šç¡®ä¿è§’è‰²IDæ­£ç¡®è®¾ç½®ä¸º4ï¼ˆæ±Ÿæˆ·å·æŸ¯å—ï¼‰
        self.character_id = 4
        
        # æŸ¯å—ç›¸å…³é¡µé¢
        self.pages_to_crawl = [
            {
                "url": "/æ±Ÿæˆ·å·æŸ¯å—",
                "title": "æ±Ÿæˆ·å·æŸ¯å—",
                "category": "MAIN_CHARACTER",
                "importance": 10
            },
            {
                "url": "/å·¥è—¤æ–°ä¸€",
                "title": "å·¥è—¤æ–°ä¸€", 
                "category": "MAIN_CHARACTER",
                "importance": 10
            },
            {
                "url": "/æ¯›åˆ©å…°",
                "title": "æ¯›åˆ©å…°",
                "category": "MAIN_CHARACTER", 
                "importance": 9
            },
            {
                "url": "/æ¯›åˆ©å°äº”éƒ",
                "title": "æ¯›åˆ©å°äº”éƒ",
                "category": "MAIN_CHARACTER",
                "importance": 8
            },
            {
                "url": "/é˜¿ç¬ åšå£«",
                "title": "é˜¿ç¬ åšå£«",
                "category": "SUPPORTING_CHARACTER",
                "importance": 8
            },
            {
                "url": "/é»‘æš—ç»„ç»‡",
                "title": "é»‘æš—ç»„ç»‡",
                "category": "ORGANIZATION",
                "importance": 9
            },
            {
                "url": "/å°‘å¹´ä¾¦æ¢å›¢",
                "title": "å°‘å¹´ä¾¦æ¢å›¢",
                "category": "ORGANIZATION",
                "importance": 7
            },
            {
                "url": "/APTX4869",
                "title": "APTX4869",
                "category": "ITEM",
                "importance": 9
            },
            {
                "url": "/å¸ä¸¹å°å­¦",
                "title": "å¸ä¸¹å°å­¦",
                "category": "LOCATION",
                "importance": 6
            },
            {
                "url": "/å¸ä¸¹é«˜ä¸­",
                "title": "å¸ä¸¹é«˜ä¸­", 
                "category": "LOCATION",
                "importance": 6
            },
            {
                "url": "/æœéƒ¨å¹³æ¬¡",
                "title": "æœéƒ¨å¹³æ¬¡",
                "category": "DETECTIVE",
                "importance": 7
            },
            {
                "url": "/æ€ªç›—åŸºå¾·",
                "title": "æ€ªç›—åŸºå¾·",
                "category": "THIEF",
                "importance": 8
            },
            {
                "url": "/æ­¥ç¾",
                "title": "å‰ç”°æ­¥ç¾",
                "category": "DETECTIVE_BOYS",
                "importance": 6
            },
            {
                "url": "/å…‰å½¦",
                "title": "åœ†è°·å…‰å½¦",
                "category": "DETECTIVE_BOYS", 
                "importance": 6
            },
            {
                "url": "/å…ƒå¤ª",
                "title": "å°å¶‹å…ƒå¤ª",
                "category": "DETECTIVE_BOYS",
                "importance": 6
            },
            {
                "url": "/ç°åŸå“€",
                "title": "ç°åŸå“€",
                "category": "MAIN_CHARACTER",
                "importance": 9
            },
            {
                "url": "/å®«é‡å¿—ä¿",
                "title": "å®«é‡å¿—ä¿",
                "category": "MAIN_CHARACTER",
                "importance": 8
            },
            {
                "url": "/FBI",
                "title": "FBI",
                "category": "ORGANIZATION",
                "importance": 7
            }
        ]
        
    def fetch_page(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            full_url = self.base_url + url
            print(f"æ­£åœ¨è·å–: {full_url}")
            
            response = requests.get(full_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            # å¤„ç†ç¼–ç 
            response.encoding = 'utf-8'
            return response.text
            
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return ""
    
    def extract_page_data(self, html: str, title: str, category: str, importance: int) -> Dict:
        """æå–é¡µé¢æ•°æ®"""
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æ ¹æ®è°ƒè¯•ç»“æœï¼Œä½¿ç”¨æœ‰æ•ˆçš„é€‰æ‹©å™¨
            content_selectors = [
                'div#mw-content-text',  # è¿™ä¸ªåŒ…å«æœ€å¤šå†…å®¹
                'div.mw-content-ltr',
                'div#content',
                'div.mw-parser-output'
            ]
            
            content_div = None
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    text_length = len(content_div.get_text().strip())
                    print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨: {selector} (åŒ…å« {text_length} å­—ç¬¦)")
                    if text_length > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
                        break
            
            if not content_div:
                print("âš ï¸ æœªæ‰¾åˆ°å†…å®¹å®¹å™¨ï¼Œå°è¯•ç›´æ¥æå–æ®µè½")
                paragraphs = soup.find_all('p')
            else:
                paragraphs = content_div.find_all('p')
            
            # æå–æ–‡æœ¬å†…å®¹ - é™ä½é•¿åº¦è¦æ±‚
            content_parts = []
            for p in paragraphs:
                text = p.get_text().strip()
                if text and len(text) > 10:  # é™ä½è¦æ±‚ä»20åˆ°10
                    # æ¸…ç†æ–‡æœ¬
                    text = re.sub(r'\[\d+\]', '', text)  # ç§»é™¤å¼•ç”¨æ ‡è®°
                    text = re.sub(r'\s+', ' ', text)  # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
                    text = re.sub(r'^[ã€‚ï¼Œã€ï¼›ï¼šï¼Ÿï¼]+', '', text)  # ç§»é™¤å¼€å¤´çš„æ ‡ç‚¹
                    if text:  # ç¡®ä¿æ¸…ç†åä»æœ‰å†…å®¹
                        content_parts.append(text)
            
            # å¦‚æœæ®µè½ä¸å¤Ÿï¼Œå°è¯•å…¶ä»–å…ƒç´ 
            if len(content_parts) < 3:
                print("âš ï¸ æ®µè½è¾ƒå°‘ï¼Œå°è¯•æå–å…¶ä»–æ–‡æœ¬å…ƒç´ ")
                # å°è¯•æå–è¡¨æ ¼ã€åˆ—è¡¨ç­‰å…¶ä»–å†…å®¹
                other_elements = content_div.find_all(['div', 'li', 'td', 'th']) if content_div else []
                for elem in other_elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 15 and text not in [part for part in content_parts]:
                        # é¿å…é‡å¤å†…å®¹
                        content_parts.append(text)
                        if len(content_parts) >= 10:  # è¶³å¤Ÿäº†å°±åœæ­¢
                            break
            
            if not content_parts:
                print(f"âš ï¸ é¡µé¢ {title} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆå†…å®¹")
                return None
                
            content = '\n\n'.join(content_parts[:15])  # å¢åŠ åˆ°15æ®µ
            
            if len(content) < 30:  # é™ä½æœ€å°è¦æ±‚
                print(f"âš ï¸ æå–å†…å®¹è¿‡çŸ­: {len(content)} å­—ç¬¦")
                print(f"å†…å®¹é¢„è§ˆ: {content[:100]}")
                return None
            
            # âš ï¸ å…³é”®ï¼šç¡®ä¿character_idæ­£ç¡®è®¾ç½®
            knowledge_item = {
                "character_id": self.character_id,  # ç¡®ä¿æ˜¯4ï¼Œä¸æ˜¯é»˜è®¤çš„1
                "title": title,
                "content": content[:2000],  # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™
                "knowledge_type": category.lower(),
                "importance_score": importance,
                "source": "conanpedia",
                "source_url": self.base_url + "/" + urllib.parse.quote(title),
                "language": "zh",
                "status": 1,
                "tags": [category, "detective", "conan", "anime"]
            }
            
            print(f"âœ… æˆåŠŸæå–: {title} ({len(content)} å­—ç¬¦)")
            return knowledge_item
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥ {title}: {e}")
            return None
    
    def crawl_all(self):
        """çˆ¬å–æ‰€æœ‰é¡µé¢"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–æŸ¯å—çŸ¥è¯†åº“ï¼Œè§’è‰²ID: {self.character_id}")
        print(f"ğŸ“‹ è®¡åˆ’çˆ¬å– {len(self.pages_to_crawl)} ä¸ªé¡µé¢")
        
        knowledge_list = []
        success_count = 0
        
        for i, page_info in enumerate(self.pages_to_crawl, 1):
            print(f"\nğŸ“„ [{i}/{len(self.pages_to_crawl)}] å¤„ç†: {page_info['title']}")
            
            html = self.fetch_page(page_info['url'])
            if html:
                knowledge_item = self.extract_page_data(
                    html, 
                    page_info['title'], 
                    page_info['category'], 
                    page_info['importance']
                )
                
                if knowledge_item:
                    knowledge_list.append(knowledge_item)
                    success_count += 1
                    
                    # ä¿å­˜å•ä¸ªæ–‡ä»¶
                    safe_filename = re.sub(r'[^\w\s-]', '', page_info['title']).strip()
                    output_file = os.path.join(self.output_dir, f"{safe_filename}.json")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(knowledge_item, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ±‡æ€»æ–‡ä»¶
        if knowledge_list:
            output_file = os.path.join(self.output_dir, "conan_knowledge.json")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(knowledge_list, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"âœ… æˆåŠŸ: {success_count}/{len(self.pages_to_crawl)} ä¸ªé¡µé¢")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(json.dumps(knowledge_list, ensure_ascii=False))} å­—ç¬¦")
            print(f"âš ï¸ è§’è‰²IDç¡®è®¤: {self.character_id} (æ±Ÿæˆ·å·æŸ¯å—)")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–ä»»ä½•å†…å®¹")

if __name__ == "__main__":
    crawler = ConanCrawler()
    crawler.crawl_all()
