#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¯å—ç™¾ç§‘æ™ºèƒ½æ‰¹é‡çˆ¬è™«
åŸºäºæå–çš„é“¾æ¥è¿›è¡Œå¤§è§„æ¨¡çŸ¥è¯†åº“æ„å»º
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import random
from typing import List, Dict, Set
import re
from urllib.parse import unquote

class SmartConanCrawler:
    """æ™ºèƒ½æŸ¯å—æ‰¹é‡çˆ¬è™«"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.base_url = "https://www.conanpedia.com"
        self.character_id = 4  # æŸ¯å—çš„è§’è‰²ID
        self.output_dir = "../../data/conan_expanded"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®çˆ¬å–å‚æ•°
        self.delay_range = (2, 5)  # è¯·æ±‚é—´éš”2-5ç§’
        self.max_pages = 200  # æœ€å¤§çˆ¬å–é¡µé¢æ•°
        self.min_content_length = 50  # æœ€å°å†…å®¹é•¿åº¦
        
        # çˆ¬å–ç»Ÿè®¡
        self.stats = {
            'total_attempted': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'duplicate': 0,
            'too_short': 0
        }
        
        # å·²çˆ¬å–çš„URLé›†åˆï¼ˆå»é‡ï¼‰
        self.crawled_urls: Set[str] = set()
        self.knowledge_items: List[Dict] = []
        
    def load_links(self, filename: str = "extracted_links.json") -> List[Dict]:
        """åŠ è½½æå–çš„é“¾æ¥"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                links = json.load(f)
            print(f"âœ… åŠ è½½äº† {len(links)} ä¸ªé“¾æ¥")
            return links
        except Exception as e:
            print(f"âŒ åŠ è½½é“¾æ¥å¤±è´¥: {e}")
            return []
    
    def prioritize_links(self, links: List[Dict]) -> List[Dict]:
        """å¯¹é“¾æ¥è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        print("ğŸ¯ å¯¹é“¾æ¥è¿›è¡Œä¼˜å…ˆçº§æ’åº...")
        
        # å®šä¹‰ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆè¶Šé‡è¦æƒé‡è¶Šé«˜ï¼‰
        priority_keywords = {
            # ä¸»è¦è§’è‰² - æœ€é«˜ä¼˜å…ˆçº§
            'æ±Ÿæˆ·å·æŸ¯å—': 100, 'å·¥è—¤æ–°ä¸€': 100, 'æ¯›åˆ©å…°': 95, 'æ¯›åˆ©å°äº”éƒ': 90,
            'ç°åŸå“€': 95, 'å®«é‡å¿—ä¿': 90, 'é˜¿ç¬ åšå£«': 85, 'æœéƒ¨å¹³æ¬¡': 85,
            'æ€ªç›—åŸºå¾·': 85, 'é»‘è¡£ç»„ç»‡': 95, 'å°‘å¹´ä¾¦æ¢å›¢': 80,
            
            # é‡è¦é…è§’ - é«˜ä¼˜å…ˆçº§  
            'ç›®æš®': 70, 'é«˜æœ¨': 70, 'ä½è—¤': 70, 'ç™½é¸Ÿ': 65, 'åƒå¶': 65,
            'ç´é…’': 80, 'ä¼ç‰¹åŠ ': 75, 'è´å°”æ‘©å¾·': 80, 'åŸºå°”': 75, 'æ³¢æœ¬': 75,
            'èµ¤äº•': 80, 'FBI': 70, 'CIA': 65,
            
            # é‡è¦å‰§é›† - ä¸­ç­‰ä¼˜å…ˆçº§
            'é»‘è¡£ç»„ç»‡': 70, 'é‡è¦': 60, 'ç‰¹åˆ«ç¯‡': 55, 'å‰§åœºç‰ˆ': 50,
            
            # æ™®é€šå†…å®¹ - è¾ƒä½ä¼˜å…ˆçº§
            'TV': 30, 'File': 25, 'é›†': 20
        }
        
        def calculate_priority(link: Dict) -> int:
            """è®¡ç®—é“¾æ¥ä¼˜å…ˆçº§"""
            text = link.get('text', '')
            url = link.get('url', '')
            
            # åŸºç¡€åˆ†æ•°
            score = 10
            
            # æ ¹æ®å…³é”®è¯åŠ åˆ†
            for keyword, weight in priority_keywords.items():
                if keyword in text or keyword in unquote(url):
                    score += weight
            
            # è§’è‰²é¡µé¢åŠ åˆ†
            if link.get('category') == 'CHARACTERS':
                score += 20
                
            # é•¿åº¦åˆç†çš„æ ‡é¢˜åŠ åˆ†
            if 3 <= len(text) <= 50:
                score += 10
            
            return score
        
        # æ’åºå¹¶é™åˆ¶æ•°é‡
        sorted_links = sorted(links, key=calculate_priority, reverse=True)
        prioritized = sorted_links[:self.max_pages]
        
        print(f"ğŸ“Š ä¼˜å…ˆçº§æ’åºå®Œæˆï¼Œé€‰æ‹©å‰ {len(prioritized)} ä¸ªé«˜ä¼˜å…ˆçº§é“¾æ¥")
        
        # æ˜¾ç¤ºå‰10ä¸ªé«˜ä¼˜å…ˆçº§é“¾æ¥
        print("\nğŸ” å‰10ä¸ªé«˜ä¼˜å…ˆçº§é“¾æ¥:")
        for i, link in enumerate(prioritized[:10], 1):
            priority = calculate_priority(link)
            print(f"  {i}. {link['text']} (ä¼˜å…ˆçº§: {priority})")
        
        return prioritized
    
    def fetch_page_content(self, url: str) -> str:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            # éšæœºå»¶è¿Ÿ
            delay = random.uniform(*self.delay_range)
            time.sleep(delay)
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            return response.text
            
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return ""
    
    def extract_knowledge(self, html: str, title: str, url: str, category: str) -> Dict:
        """æå–çŸ¥è¯†å†…å®¹"""
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ä½¿ç”¨å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                'div#mw-content-text',
                'div.mw-content-ltr', 
                'div#content',
                'div.mw-parser-output'
            ]
            
            content_div = None
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div and len(content_div.get_text().strip()) > 100:
                    break
            
            if not content_div:
                return None
            
            # æå–æ®µè½
            paragraphs = content_div.find_all('p')
            content_parts = []
            
            for p in paragraphs:
                text = p.get_text().strip()
                if text and len(text) > 15:
                    # æ¸…ç†æ–‡æœ¬
                    text = re.sub(r'\[\d+\]', '', text)  # ç§»é™¤å¼•ç”¨
                    text = re.sub(r'\s+', ' ', text)  # æ ‡å‡†åŒ–ç©ºæ ¼
                    text = re.sub(r'^[ã€‚ï¼Œã€ï¼›ï¼šï¼Ÿï¼\s]+', '', text)  # ç§»é™¤å¼€å¤´æ ‡ç‚¹
                    if text and len(text) > 10:
                        content_parts.append(text)
            
            # å¦‚æœæ®µè½ä¸å¤Ÿï¼Œæå–å…¶ä»–å…ƒç´ 
            if len(content_parts) < 3:
                other_elements = content_div.find_all(['div', 'li', 'td', 'th', 'dd'])
                for elem in other_elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 20:
                        # é¿å…é‡å¤
                        if not any(text in part for part in content_parts):
                            text = re.sub(r'\s+', ' ', text)
                            content_parts.append(text)
                            if len(content_parts) >= 10:
                                break
            
            if not content_parts:
                return None
                
            # åˆå¹¶å†…å®¹
            content = '\n\n'.join(content_parts[:20])  # å–å‰20æ®µ
            
            if len(content) < self.min_content_length:
                return None
            
            # é™åˆ¶é•¿åº¦é¿å…tokenè¶…é™
            if len(content) > 3000:
                content = content[:3000] + "..."
            
            # ç¡®å®šçŸ¥è¯†ç±»å‹
            knowledge_type = self.determine_knowledge_type(title, category, content)
            
            # è®¡ç®—é‡è¦æ€§åˆ†æ•°
            importance = self.calculate_importance(title, content)
            
            return {
                "character_id": self.character_id,
                "title": title,
                "content": content,
                "knowledge_type": knowledge_type,
                "importance_score": importance,
                "source": "conanpedia",
                "source_url": url,
                "language": "zh",
                "status": 1,
                "tags": [category.lower(), "detective", "conan", "anime", knowledge_type]
            }
            
        except Exception as e:
            print(f"âŒ å†…å®¹æå–å¤±è´¥ {title}: {e}")
            return None
    
    def determine_knowledge_type(self, title: str, category: str, content: str) -> str:
        """ç¡®å®šçŸ¥è¯†ç±»å‹"""
        # åŸºäºæ ‡é¢˜å’Œå†…å®¹åˆ¤æ–­çŸ¥è¯†ç±»å‹
        if any(keyword in title for keyword in ['TV', 'File', 'é›†', 'è¯']):
            return "episode"
        elif any(keyword in title for keyword in ['ç»„ç»‡', 'é»‘è¡£', 'FBI', 'CIA']):
            return "organization"
        elif any(keyword in title for keyword in ['ä¾¦æ¢', 'è­¦å¯Ÿ', 'åšå£«', 'åŒ»ç”Ÿ']):
            return "character"
        elif any(keyword in title for keyword in ['æ¡ˆä»¶', 'äº‹ä»¶', 'æ€äºº', 'ç»‘æ¶']):
            return "case"
        elif any(keyword in title for keyword in ['é“å…·', 'å‘æ˜', 'è¯']):
            return "item"
        elif category == "CHARACTERS":
            return "character"
        elif category == "TV_ANIME":
            return "episode"
        else:
            return "knowledge"
    
    def calculate_importance(self, title: str, content: str) -> int:
        """è®¡ç®—é‡è¦æ€§åˆ†æ•°"""
        score = 5  # åŸºç¡€åˆ†æ•°
        
        # ä¸»è¦è§’è‰²åŠ åˆ†
        main_characters = ['æ±Ÿæˆ·å·æŸ¯å—', 'å·¥è—¤æ–°ä¸€', 'æ¯›åˆ©å…°', 'æ¯›åˆ©å°äº”éƒ', 'ç°åŸå“€', 'é»‘è¡£ç»„ç»‡']
        for char in main_characters:
            if char in title:
                score += 3
                break
            elif char in content:
                score += 1
                break
        
        # å†…å®¹é•¿åº¦åŠ åˆ†
        if len(content) > 1000:
            score += 2
        elif len(content) > 500:
            score += 1
        
        # é™åˆ¶åˆ†æ•°èŒƒå›´
        return min(max(score, 1), 10)
    
    def crawl_batch(self, links: List[Dict]) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡çˆ¬å– {len(links)} ä¸ªé¡µé¢")
        print("=" * 50)
        
        for i, link in enumerate(links, 1):
            url = link['full_url']
            title = link['text']
            category = link['category']
            
            print(f"\nğŸ“„ [{i}/{len(links)}] å¤„ç†: {title}")
            
            self.stats['total_attempted'] += 1
            
            # æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
            if url in self.crawled_urls:
                print(f"â­ï¸  å·²çˆ¬å–ï¼Œè·³è¿‡")
                self.stats['skipped'] += 1
                continue
            
            # è·å–é¡µé¢å†…å®¹
            html = self.fetch_page_content(url)
            if not html:
                self.stats['failed'] += 1
                continue
            
            # æå–çŸ¥è¯†
            knowledge = self.extract_knowledge(html, title, url, category)
            if not knowledge:
                print(f"âš ï¸  å†…å®¹æå–å¤±è´¥æˆ–å¤ªçŸ­")
                self.stats['too_short'] += 1
                continue
            
            # æ£€æŸ¥é‡å¤å†…å®¹
            if self.is_duplicate_content(knowledge):
                print(f"ğŸ”„ é‡å¤å†…å®¹ï¼Œè·³è¿‡")
                self.stats['duplicate'] += 1
                continue
            
            # ä¿å­˜æˆåŠŸ
            self.knowledge_items.append(knowledge)
            self.crawled_urls.add(url)
            self.stats['success'] += 1
            
            print(f"âœ… æˆåŠŸæå–: {len(knowledge['content'])} å­—ç¬¦ï¼Œé‡è¦æ€§: {knowledge['importance_score']}")
            
            # ä¿å­˜å•ä¸ªæ–‡ä»¶
            safe_filename = re.sub(r'[^\w\s-]', '', title).strip()[:50]
            if safe_filename:
                output_file = os.path.join(self.output_dir, f"{safe_filename}.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(knowledge, f, ensure_ascii=False, indent=2)
            
            # æ˜¾ç¤ºè¿›åº¦
            success_rate = (self.stats['success'] / self.stats['total_attempted']) * 100
            print(f"ğŸ“Š è¿›åº¦: {success_rate:.1f}% æˆåŠŸç‡ ({self.stats['success']}/{self.stats['total_attempted']})")
            
            # æ¯50ä¸ªé¡µé¢ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if i % 50 == 0:
                self.save_progress()
        
        return self.knowledge_items
    
    def is_duplicate_content(self, knowledge: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤å†…å®¹"""
        new_content = knowledge['content']
        new_title = knowledge['title']
        
        for existing in self.knowledge_items:
            # æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥
            if new_title == existing['title']:
                return True
            
            # å†…å®¹ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆç®€å•ç‰ˆï¼‰
            if len(new_content) > 100 and len(existing['content']) > 100:
                # æ£€æŸ¥å‰100å­—ç¬¦çš„é‡å¤ç‡
                if new_content[:100] == existing['content'][:100]:
                    return True
        
        return False
    
    def save_progress(self):
        """ä¿å­˜çˆ¬å–è¿›åº¦"""
        if self.knowledge_items:
            progress_file = os.path.join(self.output_dir, "crawl_progress.json")
            progress_data = {
                'stats': self.stats,
                'knowledge_count': len(self.knowledge_items),
                'crawled_urls': list(self.crawled_urls),
                'latest_items': self.knowledge_items[-10:]  # æœ€æ–°10ä¸ªé¡¹ç›®
            }
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {len(self.knowledge_items)} ä¸ªçŸ¥è¯†æ¡ç›®")
    
    def save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        if not self.knowledge_items:
            print("âŒ æ²¡æœ‰æˆåŠŸçˆ¬å–ä»»ä½•å†…å®¹")
            return
        
        # ä¿å­˜å®Œæ•´çŸ¥è¯†åº“
        output_file = os.path.join(self.output_dir, "conan_expanded_knowledge.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_items, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report()
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ“„ ä¸»æ–‡ä»¶: {output_file}")
        print(f"ğŸ“Š çŸ¥è¯†æ¡ç›®: {len(self.knowledge_items)} ä¸ª")
    
    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            'crawl_summary': self.stats,
            'knowledge_stats': {
                'total_items': len(self.knowledge_items),
                'avg_content_length': sum(len(k['content']) for k in self.knowledge_items) // len(self.knowledge_items) if self.knowledge_items else 0,
                'knowledge_types': {},
                'importance_distribution': {}
            }
        }
        
        # ç»Ÿè®¡çŸ¥è¯†ç±»å‹
        for item in self.knowledge_items:
            ktype = item['knowledge_type']
            importance = item['importance_score']
            
            report['knowledge_stats']['knowledge_types'][ktype] = report['knowledge_stats']['knowledge_types'].get(ktype, 0) + 1
            report['knowledge_stats']['importance_distribution'][str(importance)] = report['knowledge_stats']['importance_distribution'].get(str(importance), 0) + 1
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, "crawl_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ çˆ¬å–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ•µï¸ æŸ¯å—ç™¾ç§‘æ™ºèƒ½æ‰¹é‡çˆ¬è™«")
    print("=" * 50)
    
    crawler = SmartConanCrawler()
    
    # åŠ è½½é“¾æ¥
    links = crawler.load_links()
    if not links:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„é“¾æ¥")
        return
    
    # ä¼˜å…ˆçº§æ’åº
    prioritized_links = crawler.prioritize_links(links)
    
    print(f"\nâš ï¸  å‡†å¤‡çˆ¬å– {len(prioritized_links)} ä¸ªé¡µé¢")
    print(f"é¢„è®¡ç”¨æ—¶: {len(prioritized_links) * 3.5 / 60:.1f} åˆ†é’Ÿ")
    
    # å¼€å§‹çˆ¬å–
    results = crawler.crawl_batch(prioritized_links)
    
    # ä¿å­˜ç»“æœ
    crawler.save_final_results()
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  å°è¯•çˆ¬å–: {crawler.stats['total_attempted']} é¡µ")
    print(f"  æˆåŠŸ: {crawler.stats['success']} é¡µ")
    print(f"  å¤±è´¥: {crawler.stats['failed']} é¡µ")
    print(f"  è·³è¿‡: {crawler.stats['skipped']} é¡µ")
    print(f"  é‡å¤: {crawler.stats['duplicate']} é¡µ")
    print(f"  å†…å®¹è¿‡çŸ­: {crawler.stats['too_short']} é¡µ")
    print(f"  æˆåŠŸç‡: {(crawler.stats['success'] / crawler.stats['total_attempted'] * 100):.1f}%")

if __name__ == "__main__":
    main()
